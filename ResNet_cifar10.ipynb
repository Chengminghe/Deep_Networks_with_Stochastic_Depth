{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "chief-spine",
   "metadata": {},
   "source": [
    "## Training of CIFAR-10 using 110-layer ResNet with constant depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utils.neuralnets.ResNet110 import ResNet110\n",
    "import time\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D,Input,Flatten,Dense\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "personalized-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load data and standardize cifar10\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = (x_train - np.mean(x_train,axis=0))/np.std(x_train,axis=0)\n",
    "x_test = (x_test - np.mean(x_test,axis=0))/np.std(x_test,axis=0)\n",
    "##train validation split, 45000 for training and 5000 for validation\n",
    "np.random.seed(42)\n",
    "mask_val = np.random.choice(50000,5000,replace=False)\n",
    "mask_train = np.array([i for i in range(50000) if i not in mask_val])\n",
    "x_val, y_val = x_train[mask_val], y_train[mask_val]\n",
    "x_train, y_train = x_train[mask_train], y_train[mask_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "environmental-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "##data augmentation\n",
    "##augmented with horizontal flip,random erasing and random shift by 0.1\n",
    "def randomErasing(x, cut_size=16):\n",
    "    x = np.copy(x)\n",
    "    fill = x.mean()\n",
    "\n",
    "    h, w, _ = x.shape\n",
    "    top = np.random.randint(0 - cut_size // 2, h - cut_size)\n",
    "    left = np.random.randint(0 - cut_size // 2, w - cut_size)\n",
    "    bottom = top + cut_size\n",
    "    right = left + cut_size\n",
    "    if top < 0:\n",
    "        top = 0\n",
    "    if left < 0:\n",
    "        left = 0\n",
    "    x[top:bottom, left:right, :].fill(fill)\n",
    "    return x\n",
    "batch_size = 128\n",
    "datagen_for_train = ImageDataGenerator(horizontal_flip=True,width_shift_range= 0.1, height_shift_range= 0.1\n",
    "                                       ,preprocessing_function = randomErasing)\n",
    "datagen_for_test = ImageDataGenerator()\n",
    "train_data = datagen_for_train.flow(x_train,y_train,batch_size=batch_size)\n",
    "validation_data = datagen_for_test.flow(x_val, y_val, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "martial-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create a ResNet110 model\n",
    "input_shape = x_train.shape[1:]\n",
    "num_class = 10\n",
    "model = ResNet110(input_shape=input_shape,num_class=num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wound-duplicate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "Training loss at step 0: 9.9470\n",
      "Training loss at step 25: 6.9868\n",
      "Training loss at step 50: 3.1300\n",
      "Training loss at step 75: 2.8975\n",
      "Training loss at step 100: 2.7906\n",
      "Training loss at step 125: 2.2324\n",
      "Training loss at step 150: 2.5272\n",
      "Training loss at step 175: 2.1978\n",
      "Training loss at step 200: 2.4153\n",
      "Training loss at step 225: 2.1671\n",
      "Training loss at step 250: 1.9893\n",
      "Training loss at step 275: 2.1555\n",
      "Training loss at step 300: 1.9511\n",
      "Training loss at step 325: 2.0011\n",
      "Training loss at step 350: 1.8480\n",
      "Training accuracy: 0.1788 Validation accuracy: 0.2406 Time taken: 155.12s\n",
      "Epoch 2/200\n",
      "Training loss at step 0: 2.0163\n",
      "Training loss at step 25: 2.0813\n",
      "Training loss at step 50: 1.9330\n",
      "Training loss at step 75: 1.8795\n",
      "Training loss at step 100: 2.0079\n",
      "Training loss at step 125: 2.0072\n",
      "Training loss at step 150: 1.9356\n",
      "Training loss at step 175: 1.6245\n",
      "Training loss at step 200: 1.9060\n",
      "Training loss at step 225: 1.9143\n",
      "Training loss at step 250: 1.7622\n",
      "Training loss at step 275: 1.8085\n",
      "Training loss at step 300: 2.0287\n",
      "Training loss at step 325: 1.7931\n",
      "Training loss at step 350: 1.6659\n",
      "Training accuracy: 0.3147 Validation accuracy: 0.3062 Time taken: 123.08s\n",
      "Epoch 3/200\n",
      "Training loss at step 0: 2.3017\n",
      "Training loss at step 25: 1.8138\n",
      "Training loss at step 50: 1.6504\n",
      "Training loss at step 75: 1.5615\n",
      "Training loss at step 100: 1.8893\n",
      "Training loss at step 125: 1.6701\n",
      "Training loss at step 150: 1.6482\n",
      "Training loss at step 175: 1.7602\n",
      "Training loss at step 200: 1.5989\n",
      "Training loss at step 225: 1.6682\n",
      "Training loss at step 250: 1.8739\n",
      "Training loss at step 275: 1.8401\n",
      "Training loss at step 300: 1.6495\n",
      "Training loss at step 325: 1.7143\n",
      "Training loss at step 350: 1.8338\n",
      "Training accuracy: 0.3795 Validation accuracy: 0.3800 Time taken: 122.72s\n",
      "Epoch 4/200\n",
      "Training loss at step 0: 1.7581\n",
      "Training loss at step 25: 1.5942\n",
      "Training loss at step 50: 1.5681\n",
      "Training loss at step 75: 1.7329\n",
      "Training loss at step 100: 1.4741\n",
      "Training loss at step 125: 1.6507\n",
      "Training loss at step 150: 1.4731\n",
      "Training loss at step 175: 1.7041\n",
      "Training loss at step 200: 1.5529\n",
      "Training loss at step 225: 1.6024\n",
      "Training loss at step 250: 1.5335\n",
      "Training loss at step 275: 1.6920\n",
      "Training loss at step 300: 1.6047\n",
      "Training loss at step 325: 1.5341\n",
      "Training loss at step 350: 1.5004\n",
      "Training accuracy: 0.4143 Validation accuracy: 0.4274 Time taken: 122.54s\n",
      "Epoch 5/200\n",
      "Training loss at step 0: 1.7993\n",
      "Training loss at step 25: 1.4283\n",
      "Training loss at step 50: 1.7467\n",
      "Training loss at step 75: 1.4365\n",
      "Training loss at step 100: 1.5953\n",
      "Training loss at step 125: 1.5860\n",
      "Training loss at step 150: 1.5399\n",
      "Training loss at step 175: 1.5154\n",
      "Training loss at step 200: 1.4299\n",
      "Training loss at step 225: 1.5644\n",
      "Training loss at step 250: 1.5077\n",
      "Training loss at step 275: 1.4986\n",
      "Training loss at step 300: 1.5495\n",
      "Training loss at step 325: 1.4342\n",
      "Training loss at step 350: 1.5148\n",
      "Training accuracy: 0.4475 Validation accuracy: 0.4380 Time taken: 122.90s\n",
      "Epoch 6/200\n",
      "Training loss at step 0: 1.4590\n",
      "Training loss at step 25: 1.4200\n",
      "Training loss at step 50: 1.3848\n",
      "Training loss at step 75: 1.5987\n",
      "Training loss at step 100: 1.4209\n",
      "Training loss at step 125: 1.3207\n",
      "Training loss at step 150: 1.5117\n",
      "Training loss at step 175: 1.4471\n",
      "Training loss at step 200: 1.4955\n",
      "Training loss at step 225: 1.2741\n",
      "Training loss at step 250: 1.5074\n",
      "Training loss at step 275: 1.2314\n",
      "Training loss at step 300: 1.3968\n",
      "Training loss at step 325: 1.2976\n",
      "Training loss at step 350: 1.3169\n",
      "Training accuracy: 0.4793 Validation accuracy: 0.4240 Time taken: 123.04s\n",
      "Epoch 7/200\n",
      "Training loss at step 0: 1.6461\n",
      "Training loss at step 25: 1.4719\n",
      "Training loss at step 50: 1.3019\n",
      "Training loss at step 75: 1.4678\n",
      "Training loss at step 100: 1.4803\n",
      "Training loss at step 125: 1.4879\n",
      "Training loss at step 150: 1.2133\n",
      "Training loss at step 175: 1.2389\n",
      "Training loss at step 200: 1.3286\n",
      "Training loss at step 225: 1.3884\n",
      "Training loss at step 250: 1.2535\n",
      "Training loss at step 275: 1.3451\n",
      "Training loss at step 300: 1.0554\n",
      "Training loss at step 325: 1.3517\n",
      "Training loss at step 350: 1.5037\n",
      "Training accuracy: 0.5100 Validation accuracy: 0.4568 Time taken: 123.18s\n",
      "Epoch 8/200\n",
      "Training loss at step 0: 1.5977\n",
      "Training loss at step 25: 1.3805\n",
      "Training loss at step 50: 1.3543\n",
      "Training loss at step 75: 1.2889\n",
      "Training loss at step 100: 1.2932\n",
      "Training loss at step 125: 1.3732\n",
      "Training loss at step 150: 1.2492\n",
      "Training loss at step 175: 1.4088\n",
      "Training loss at step 200: 1.2308\n",
      "Training loss at step 225: 1.2661\n",
      "Training loss at step 250: 1.2105\n",
      "Training loss at step 275: 1.1422\n",
      "Training loss at step 300: 1.4890\n",
      "Training loss at step 325: 1.3424\n",
      "Training loss at step 350: 1.1260\n",
      "Training accuracy: 0.5379 Validation accuracy: 0.5026 Time taken: 123.65s\n",
      "Epoch 9/200\n",
      "Training loss at step 0: 1.3336\n",
      "Training loss at step 25: 1.3050\n",
      "Training loss at step 50: 1.1818\n",
      "Training loss at step 75: 1.3111\n",
      "Training loss at step 100: 1.3558\n",
      "Training loss at step 125: 1.1262\n",
      "Training loss at step 150: 1.0994\n",
      "Training loss at step 175: 1.1798\n",
      "Training loss at step 200: 1.1804\n",
      "Training loss at step 225: 1.3403\n",
      "Training loss at step 250: 1.1705\n",
      "Training loss at step 275: 1.2617\n",
      "Training loss at step 300: 1.1046\n",
      "Training loss at step 325: 1.1445\n",
      "Training loss at step 350: 1.2492\n",
      "Training accuracy: 0.5614 Validation accuracy: 0.6066 Time taken: 123.17s\n",
      "Epoch 10/200\n",
      "Training loss at step 0: 1.0579\n",
      "Training loss at step 25: 1.1671\n",
      "Training loss at step 50: 1.0830\n",
      "Training loss at step 75: 1.1472\n",
      "Training loss at step 100: 1.0256\n",
      "Training loss at step 125: 1.1899\n",
      "Training loss at step 150: 1.4006\n",
      "Training loss at step 175: 1.2358\n",
      "Training loss at step 200: 1.1484\n",
      "Training loss at step 225: 1.0818\n",
      "Training loss at step 250: 1.1064\n",
      "Training loss at step 275: 1.0547\n",
      "Training loss at step 300: 1.1285\n",
      "Training loss at step 325: 1.1016\n",
      "Training loss at step 350: 1.1804\n",
      "Training accuracy: 0.5814 Validation accuracy: 0.5006 Time taken: 123.09s\n",
      "Epoch 11/200\n",
      "Training loss at step 0: 1.2738\n",
      "Training loss at step 25: 1.1617\n",
      "Training loss at step 50: 1.2346\n",
      "Training loss at step 75: 1.1883\n",
      "Training loss at step 100: 1.1293\n",
      "Training loss at step 125: 0.9727\n",
      "Training loss at step 150: 1.2394\n",
      "Training loss at step 175: 1.0461\n",
      "Training loss at step 200: 0.9760\n",
      "Training loss at step 225: 1.0735\n",
      "Training loss at step 250: 0.9753\n",
      "Training loss at step 275: 1.3109\n",
      "Training loss at step 300: 1.1779\n",
      "Training loss at step 325: 1.0770\n",
      "Training loss at step 350: 1.1812\n",
      "Training accuracy: 0.6006 Validation accuracy: 0.6188 Time taken: 123.43s\n",
      "Epoch 12/200\n",
      "Training loss at step 0: 1.0296\n",
      "Training loss at step 25: 1.0194\n",
      "Training loss at step 50: 1.0390\n",
      "Training loss at step 75: 1.0584\n",
      "Training loss at step 100: 1.0162\n",
      "Training loss at step 125: 1.0305\n",
      "Training loss at step 150: 0.9343\n",
      "Training loss at step 175: 1.2042\n",
      "Training loss at step 200: 1.0529\n",
      "Training loss at step 225: 1.0426\n",
      "Training loss at step 250: 1.0435\n",
      "Training loss at step 275: 1.0825\n",
      "Training loss at step 300: 1.0964\n",
      "Training loss at step 325: 0.9665\n",
      "Training loss at step 350: 1.0650\n",
      "Training accuracy: 0.6170 Validation accuracy: 0.5682 Time taken: 122.43s\n",
      "Epoch 13/200\n",
      "Training loss at step 0: 1.1152\n",
      "Training loss at step 25: 1.2428\n",
      "Training loss at step 50: 0.9395\n",
      "Training loss at step 75: 1.3125\n",
      "Training loss at step 100: 1.1322\n",
      "Training loss at step 125: 1.0088\n",
      "Training loss at step 150: 0.9474\n",
      "Training loss at step 175: 0.9799\n",
      "Training loss at step 200: 1.1721\n",
      "Training loss at step 225: 1.0037\n",
      "Training loss at step 250: 1.2145\n",
      "Training loss at step 275: 1.0735\n",
      "Training loss at step 300: 1.0678\n",
      "Training loss at step 325: 0.9397\n",
      "Training loss at step 350: 1.0189\n",
      "Training accuracy: 0.6330 Validation accuracy: 0.4996 Time taken: 123.56s\n",
      "Epoch 14/200\n",
      "Training loss at step 0: 1.1818\n",
      "Training loss at step 25: 1.0778\n",
      "Training loss at step 50: 0.8246\n",
      "Training loss at step 75: 1.2922\n",
      "Training loss at step 100: 0.9651\n",
      "Training loss at step 125: 0.9783\n",
      "Training loss at step 150: 0.8770\n",
      "Training loss at step 175: 1.1320\n",
      "Training loss at step 200: 1.0282\n",
      "Training loss at step 225: 0.9781\n",
      "Training loss at step 250: 0.8832\n",
      "Training loss at step 275: 1.0001\n",
      "Training loss at step 300: 0.9568\n",
      "Training loss at step 325: 1.1744\n",
      "Training loss at step 350: 0.7993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.6457 Validation accuracy: 0.6276 Time taken: 123.22s\n",
      "Epoch 15/200\n",
      "Training loss at step 0: 1.0027\n",
      "Training loss at step 25: 0.9520\n",
      "Training loss at step 50: 0.8631\n",
      "Training loss at step 75: 0.9750\n",
      "Training loss at step 100: 0.9187\n",
      "Training loss at step 125: 0.9702\n",
      "Training loss at step 150: 0.9126\n",
      "Training loss at step 175: 1.1108\n",
      "Training loss at step 200: 1.0099\n",
      "Training loss at step 225: 1.1258\n",
      "Training loss at step 250: 0.8827\n",
      "Training loss at step 275: 0.8545\n",
      "Training loss at step 300: 0.8620\n",
      "Training loss at step 325: 1.0162\n",
      "Training loss at step 350: 0.9835\n",
      "Training accuracy: 0.6572 Validation accuracy: 0.6610 Time taken: 123.67s\n",
      "Epoch 16/200\n",
      "Training loss at step 0: 1.0963\n",
      "Training loss at step 25: 0.8843\n",
      "Training loss at step 50: 0.9002\n",
      "Training loss at step 75: 0.9264\n",
      "Training loss at step 100: 1.0090\n",
      "Training loss at step 125: 0.9188\n",
      "Training loss at step 150: 1.0734\n",
      "Training loss at step 175: 0.9363\n",
      "Training loss at step 200: 0.8411\n",
      "Training loss at step 225: 0.8022\n",
      "Training loss at step 250: 0.8102\n",
      "Training loss at step 275: 0.9796\n",
      "Training loss at step 300: 0.7577\n",
      "Training loss at step 325: 0.7610\n",
      "Training loss at step 350: 0.8186\n",
      "Training accuracy: 0.6693 Validation accuracy: 0.6322 Time taken: 123.55s\n",
      "Epoch 17/200\n",
      "Training loss at step 0: 0.9303\n",
      "Training loss at step 25: 0.8587\n",
      "Training loss at step 50: 0.8764\n",
      "Training loss at step 75: 0.8818\n",
      "Training loss at step 100: 1.0625\n",
      "Training loss at step 125: 0.9109\n",
      "Training loss at step 150: 1.0721\n",
      "Training loss at step 175: 0.8644\n",
      "Training loss at step 200: 0.8445\n",
      "Training loss at step 225: 0.9524\n",
      "Training loss at step 250: 0.8519\n",
      "Training loss at step 275: 0.8243\n",
      "Training loss at step 300: 0.6377\n",
      "Training loss at step 325: 0.9782\n",
      "Training loss at step 350: 0.9052\n",
      "Training accuracy: 0.6828 Validation accuracy: 0.5808 Time taken: 123.08s\n",
      "Epoch 18/200\n",
      "Training loss at step 0: 0.8790\n",
      "Training loss at step 25: 0.8490\n",
      "Training loss at step 50: 0.7795\n",
      "Training loss at step 75: 0.8560\n",
      "Training loss at step 100: 0.7590\n",
      "Training loss at step 125: 0.8858\n",
      "Training loss at step 150: 0.7355\n",
      "Training loss at step 175: 0.7896\n",
      "Training loss at step 200: 0.8956\n",
      "Training loss at step 225: 0.9292\n",
      "Training loss at step 250: 0.8796\n",
      "Training loss at step 275: 0.8299\n",
      "Training loss at step 300: 0.8328\n",
      "Training loss at step 325: 0.8526\n",
      "Training loss at step 350: 0.6756\n",
      "Training accuracy: 0.6880 Validation accuracy: 0.6444 Time taken: 123.50s\n",
      "Epoch 19/200\n",
      "Training loss at step 0: 0.7767\n",
      "Training loss at step 25: 0.8639\n",
      "Training loss at step 50: 0.8757\n",
      "Training loss at step 75: 0.7803\n",
      "Training loss at step 100: 0.7237\n",
      "Training loss at step 125: 0.8826\n",
      "Training loss at step 150: 0.8653\n",
      "Training loss at step 175: 0.7348\n",
      "Training loss at step 200: 0.8739\n",
      "Training loss at step 225: 0.9712\n",
      "Training loss at step 250: 0.8950\n",
      "Training loss at step 275: 0.9784\n",
      "Training loss at step 300: 0.9095\n",
      "Training loss at step 325: 0.6395\n",
      "Training loss at step 350: 0.8900\n",
      "Training accuracy: 0.7004 Validation accuracy: 0.6936 Time taken: 123.62s\n",
      "Epoch 20/200\n",
      "Training loss at step 0: 0.8576\n",
      "Training loss at step 25: 0.8490\n",
      "Training loss at step 50: 0.8944\n",
      "Training loss at step 75: 0.9522\n",
      "Training loss at step 100: 0.6348\n",
      "Training loss at step 125: 0.6670\n",
      "Training loss at step 150: 0.7720\n",
      "Training loss at step 175: 0.8274\n",
      "Training loss at step 200: 0.8696\n",
      "Training loss at step 225: 0.7908\n",
      "Training loss at step 250: 0.7892\n",
      "Training loss at step 275: 0.9434\n",
      "Training loss at step 300: 0.8029\n",
      "Training loss at step 325: 0.9414\n",
      "Training loss at step 350: 0.6439\n",
      "Training accuracy: 0.7066 Validation accuracy: 0.7298 Time taken: 123.26s\n",
      "Epoch 21/200\n",
      "Training loss at step 0: 0.7787\n",
      "Training loss at step 25: 0.6896\n",
      "Training loss at step 50: 0.7604\n",
      "Training loss at step 75: 0.7580\n",
      "Training loss at step 100: 1.0305\n",
      "Training loss at step 125: 0.6993\n",
      "Training loss at step 150: 0.6331\n",
      "Training loss at step 175: 0.7969\n",
      "Training loss at step 200: 0.9194\n",
      "Training loss at step 225: 0.8423\n",
      "Training loss at step 250: 0.8153\n",
      "Training loss at step 275: 0.7549\n",
      "Training loss at step 300: 0.8639\n",
      "Training loss at step 325: 1.0006\n",
      "Training loss at step 350: 0.7491\n",
      "Training accuracy: 0.7158 Validation accuracy: 0.6904 Time taken: 123.36s\n",
      "Epoch 22/200\n",
      "Training loss at step 0: 0.8420\n",
      "Training loss at step 25: 0.8061\n",
      "Training loss at step 50: 0.7253\n",
      "Training loss at step 75: 0.7202\n",
      "Training loss at step 100: 0.9653\n",
      "Training loss at step 125: 0.8582\n",
      "Training loss at step 150: 0.9903\n",
      "Training loss at step 175: 0.7837\n",
      "Training loss at step 200: 0.8331\n",
      "Training loss at step 225: 0.8067\n",
      "Training loss at step 250: 0.8791\n",
      "Training loss at step 275: 0.8058\n",
      "Training loss at step 300: 0.8336\n",
      "Training loss at step 325: 0.7935\n",
      "Training loss at step 350: 0.8232\n",
      "Training accuracy: 0.7215 Validation accuracy: 0.7518 Time taken: 123.21s\n",
      "Epoch 23/200\n",
      "Training loss at step 0: 0.8911\n",
      "Training loss at step 25: 0.7009\n",
      "Training loss at step 50: 0.7543\n",
      "Training loss at step 75: 0.8670\n",
      "Training loss at step 100: 0.6558\n",
      "Training loss at step 125: 0.9443\n",
      "Training loss at step 150: 0.9419\n",
      "Training loss at step 175: 0.8189\n",
      "Training loss at step 200: 0.7338\n",
      "Training loss at step 225: 0.7919\n",
      "Training loss at step 250: 0.7006\n",
      "Training loss at step 275: 0.6704\n",
      "Training loss at step 300: 0.7834\n",
      "Training loss at step 325: 0.7273\n",
      "Training loss at step 350: 0.8311\n",
      "Training accuracy: 0.7278 Validation accuracy: 0.6584 Time taken: 123.07s\n",
      "Epoch 24/200\n",
      "Training loss at step 0: 0.8903\n",
      "Training loss at step 25: 0.5615\n",
      "Training loss at step 50: 0.6704\n",
      "Training loss at step 75: 0.8348\n",
      "Training loss at step 100: 0.7125\n",
      "Training loss at step 125: 0.9459\n",
      "Training loss at step 150: 0.8704\n",
      "Training loss at step 175: 0.7890\n",
      "Training loss at step 200: 0.7400\n",
      "Training loss at step 225: 0.6159\n",
      "Training loss at step 250: 0.7893\n",
      "Training loss at step 275: 0.8585\n",
      "Training loss at step 300: 0.8157\n",
      "Training loss at step 325: 0.7881\n",
      "Training loss at step 350: 0.7572\n",
      "Training accuracy: 0.7318 Validation accuracy: 0.6928 Time taken: 123.41s\n",
      "Epoch 25/200\n",
      "Training loss at step 0: 0.6160\n",
      "Training loss at step 25: 0.7511\n",
      "Training loss at step 50: 0.6274\n",
      "Training loss at step 75: 0.6348\n",
      "Training loss at step 100: 0.5687\n",
      "Training loss at step 125: 0.6784\n",
      "Training loss at step 150: 0.6589\n",
      "Training loss at step 175: 0.7014\n",
      "Training loss at step 200: 0.8802\n",
      "Training loss at step 225: 0.7217\n",
      "Training loss at step 250: 0.7024\n",
      "Training loss at step 275: 0.7401\n",
      "Training loss at step 300: 0.6421\n",
      "Training loss at step 325: 0.7655\n",
      "Training loss at step 350: 0.6073\n",
      "Training accuracy: 0.7402 Validation accuracy: 0.6790 Time taken: 122.50s\n",
      "Epoch 26/200\n",
      "Training loss at step 0: 0.8125\n",
      "Training loss at step 25: 0.6844\n",
      "Training loss at step 50: 0.7602\n",
      "Training loss at step 75: 0.6706\n",
      "Training loss at step 100: 0.6320\n",
      "Training loss at step 125: 1.0137\n",
      "Training loss at step 150: 0.6423\n",
      "Training loss at step 175: 0.6653\n",
      "Training loss at step 200: 0.7046\n",
      "Training loss at step 225: 0.7695\n",
      "Training loss at step 250: 0.8583\n",
      "Training loss at step 275: 0.5940\n",
      "Training loss at step 300: 0.6312\n",
      "Training loss at step 325: 0.6593\n",
      "Training loss at step 350: 0.8707\n",
      "Training accuracy: 0.7434 Validation accuracy: 0.6502 Time taken: 123.08s\n",
      "Epoch 27/200\n",
      "Training loss at step 0: 0.5240\n",
      "Training loss at step 25: 0.6095\n",
      "Training loss at step 50: 0.8033\n",
      "Training loss at step 75: 0.7155\n",
      "Training loss at step 100: 0.7017\n",
      "Training loss at step 125: 0.7802\n",
      "Training loss at step 150: 0.7260\n",
      "Training loss at step 175: 0.6408\n",
      "Training loss at step 200: 0.7040\n",
      "Training loss at step 225: 0.7027\n",
      "Training loss at step 250: 0.8053\n",
      "Training loss at step 275: 0.6892\n",
      "Training loss at step 300: 0.5845\n",
      "Training loss at step 325: 0.6741\n",
      "Training loss at step 350: 0.7966\n",
      "Training accuracy: 0.7518 Validation accuracy: 0.6904 Time taken: 123.46s\n",
      "Epoch 28/200\n",
      "Training loss at step 0: 0.6600\n",
      "Training loss at step 25: 0.7523\n",
      "Training loss at step 50: 0.5237\n",
      "Training loss at step 75: 0.8563\n",
      "Training loss at step 100: 0.7542\n",
      "Training loss at step 125: 0.9837\n",
      "Training loss at step 150: 0.7123\n",
      "Training loss at step 175: 0.7220\n",
      "Training loss at step 200: 0.6679\n",
      "Training loss at step 225: 0.6751\n",
      "Training loss at step 250: 0.7135\n",
      "Training loss at step 275: 0.6320\n",
      "Training loss at step 300: 0.5819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 325: 0.6909\n",
      "Training loss at step 350: 0.8383\n",
      "Training accuracy: 0.7454 Validation accuracy: 0.6580 Time taken: 123.51s\n",
      "Epoch 29/200\n",
      "Training loss at step 0: 0.8775\n",
      "Training loss at step 25: 0.6231\n",
      "Training loss at step 50: 0.6947\n",
      "Training loss at step 75: 0.6639\n",
      "Training loss at step 100: 0.5829\n",
      "Training loss at step 125: 0.6650\n",
      "Training loss at step 150: 0.7036\n",
      "Training loss at step 175: 0.7614\n",
      "Training loss at step 200: 0.6966\n",
      "Training loss at step 225: 0.8499\n",
      "Training loss at step 250: 0.9639\n",
      "Training loss at step 275: 0.6774\n",
      "Training loss at step 300: 0.6880\n",
      "Training loss at step 325: 0.5818\n",
      "Training loss at step 350: 0.8474\n",
      "Training accuracy: 0.7560 Validation accuracy: 0.7204 Time taken: 123.22s\n",
      "Epoch 30/200\n",
      "Training loss at step 0: 0.8281\n",
      "Training loss at step 25: 0.7259\n",
      "Training loss at step 50: 0.6267\n",
      "Training loss at step 75: 0.6735\n",
      "Training loss at step 100: 0.5457\n",
      "Training loss at step 125: 0.5954\n",
      "Training loss at step 150: 0.6937\n",
      "Training loss at step 175: 0.7498\n",
      "Training loss at step 200: 0.7627\n",
      "Training loss at step 225: 0.6752\n",
      "Training loss at step 250: 0.6057\n",
      "Training loss at step 275: 0.7040\n",
      "Training loss at step 300: 0.4515\n",
      "Training loss at step 325: 0.6182\n",
      "Training loss at step 350: 0.5816\n",
      "Training accuracy: 0.7641 Validation accuracy: 0.7544 Time taken: 123.53s\n",
      "Epoch 31/200\n",
      "Training loss at step 0: 0.6353\n",
      "Training loss at step 25: 0.6011\n",
      "Training loss at step 50: 0.5304\n",
      "Training loss at step 75: 0.6900\n",
      "Training loss at step 100: 0.5905\n",
      "Training loss at step 125: 0.6302\n",
      "Training loss at step 150: 0.5822\n",
      "Training loss at step 175: 0.6885\n",
      "Training loss at step 200: 0.7100\n",
      "Training loss at step 225: 0.5957\n",
      "Training loss at step 250: 0.8439\n",
      "Training loss at step 275: 0.5018\n",
      "Training loss at step 300: 0.6907\n",
      "Training loss at step 325: 0.7000\n",
      "Training loss at step 350: 0.7481\n",
      "Training accuracy: 0.7652 Validation accuracy: 0.7744 Time taken: 123.87s\n",
      "Epoch 32/200\n",
      "Training loss at step 0: 0.5825\n",
      "Training loss at step 25: 0.5608\n",
      "Training loss at step 50: 0.7010\n",
      "Training loss at step 75: 0.5739\n",
      "Training loss at step 100: 0.5014\n",
      "Training loss at step 125: 0.6460\n",
      "Training loss at step 150: 0.5195\n",
      "Training loss at step 175: 0.7775\n",
      "Training loss at step 200: 0.7311\n",
      "Training loss at step 225: 0.5991\n",
      "Training loss at step 250: 0.5428\n",
      "Training loss at step 275: 0.6261\n",
      "Training loss at step 300: 0.8281\n",
      "Training loss at step 325: 0.6517\n",
      "Training loss at step 350: 0.7204\n",
      "Training accuracy: 0.7699 Validation accuracy: 0.7812 Time taken: 123.30s\n",
      "Epoch 33/200\n",
      "Training loss at step 0: 0.6965\n",
      "Training loss at step 25: 0.6338\n",
      "Training loss at step 50: 0.4934\n",
      "Training loss at step 75: 0.5080\n",
      "Training loss at step 100: 0.6573\n",
      "Training loss at step 125: 0.5762\n",
      "Training loss at step 150: 0.8013\n",
      "Training loss at step 175: 0.7720\n",
      "Training loss at step 200: 0.7370\n",
      "Training loss at step 225: 0.6403\n",
      "Training loss at step 250: 0.6657\n",
      "Training loss at step 275: 0.7584\n",
      "Training loss at step 300: 0.7443\n",
      "Training loss at step 325: 0.6769\n",
      "Training loss at step 350: 0.6416\n",
      "Training accuracy: 0.7707 Validation accuracy: 0.3848 Time taken: 123.08s\n",
      "Epoch 34/200\n",
      "Training loss at step 0: 0.6926\n",
      "Training loss at step 25: 0.7444\n",
      "Training loss at step 50: 0.5324\n",
      "Training loss at step 75: 0.5611\n",
      "Training loss at step 100: 0.7559\n",
      "Training loss at step 125: 0.5143\n",
      "Training loss at step 150: 0.6066\n",
      "Training loss at step 175: 0.7416\n",
      "Training loss at step 200: 0.6120\n",
      "Training loss at step 225: 0.7737\n",
      "Training loss at step 250: 0.7860\n",
      "Training loss at step 275: 0.6222\n",
      "Training loss at step 300: 0.7240\n",
      "Training loss at step 325: 0.6453\n",
      "Training loss at step 350: 0.6094\n",
      "Training accuracy: 0.7683 Validation accuracy: 0.7696 Time taken: 122.73s\n",
      "Epoch 35/200\n",
      "Training loss at step 0: 0.5836\n",
      "Training loss at step 25: 0.7115\n",
      "Training loss at step 50: 0.5917\n",
      "Training loss at step 75: 0.6842\n",
      "Training loss at step 100: 0.6646\n",
      "Training loss at step 125: 0.6136\n",
      "Training loss at step 150: 0.5802\n",
      "Training loss at step 175: 0.6324\n",
      "Training loss at step 200: 0.8961\n",
      "Training loss at step 225: 0.7576\n",
      "Training loss at step 250: 0.6044\n",
      "Training loss at step 275: 0.6100\n",
      "Training loss at step 300: 0.8986\n",
      "Training loss at step 325: 0.7240\n",
      "Training loss at step 350: 0.5641\n",
      "Training accuracy: 0.7687 Validation accuracy: 0.7476 Time taken: 123.10s\n",
      "Epoch 36/200\n",
      "Training loss at step 0: 0.4612\n",
      "Training loss at step 25: 0.6963\n",
      "Training loss at step 50: 0.7615\n",
      "Training loss at step 75: 0.5019\n",
      "Training loss at step 100: 0.8228\n",
      "Training loss at step 125: 0.6304\n",
      "Training loss at step 150: 0.6877\n",
      "Training loss at step 175: 0.5416\n",
      "Training loss at step 200: 0.6093\n",
      "Training loss at step 225: 0.7264\n",
      "Training loss at step 250: 0.6957\n",
      "Training loss at step 275: 0.5375\n",
      "Training loss at step 300: 0.8662\n",
      "Training loss at step 325: 0.5210\n",
      "Training loss at step 350: 0.5314\n",
      "Training accuracy: 0.7824 Validation accuracy: 0.7756 Time taken: 122.94s\n",
      "Epoch 37/200\n",
      "Training loss at step 0: 0.5400\n",
      "Training loss at step 25: 0.6335\n",
      "Training loss at step 50: 0.4871\n",
      "Training loss at step 75: 0.5776\n",
      "Training loss at step 100: 0.5527\n",
      "Training loss at step 125: 0.6899\n",
      "Training loss at step 150: 0.6411\n",
      "Training loss at step 175: 0.5055\n",
      "Training loss at step 200: 0.5855\n",
      "Training loss at step 225: 0.6511\n",
      "Training loss at step 250: 0.5342\n",
      "Training loss at step 275: 0.5357\n",
      "Training loss at step 300: 0.6915\n",
      "Training loss at step 325: 0.6426\n",
      "Training loss at step 350: 0.5828\n",
      "Training accuracy: 0.7878 Validation accuracy: 0.7914 Time taken: 122.52s\n",
      "Epoch 38/200\n",
      "Training loss at step 0: 0.6723\n",
      "Training loss at step 25: 0.7040\n",
      "Training loss at step 50: 0.5537\n",
      "Training loss at step 75: 0.6784\n",
      "Training loss at step 100: 0.4614\n",
      "Training loss at step 125: 0.5344\n",
      "Training loss at step 150: 0.7293\n",
      "Training loss at step 175: 0.7592\n",
      "Training loss at step 200: 0.5360\n",
      "Training loss at step 225: 0.7273\n",
      "Training loss at step 250: 0.5811\n",
      "Training loss at step 275: 0.6078\n",
      "Training loss at step 300: 0.5498\n",
      "Training loss at step 325: 0.5688\n",
      "Training loss at step 350: 0.7828\n",
      "Training accuracy: 0.7897 Validation accuracy: 0.6970 Time taken: 123.20s\n",
      "Epoch 39/200\n",
      "Training loss at step 0: 0.5511\n",
      "Training loss at step 25: 0.5492\n",
      "Training loss at step 50: 0.7323\n",
      "Training loss at step 75: 0.6017\n",
      "Training loss at step 100: 0.5261\n",
      "Training loss at step 125: 0.4896\n",
      "Training loss at step 150: 0.5841\n",
      "Training loss at step 175: 0.6811\n",
      "Training loss at step 200: 0.5948\n",
      "Training loss at step 225: 0.5951\n",
      "Training loss at step 250: 0.4944\n",
      "Training loss at step 275: 0.7070\n",
      "Training loss at step 300: 0.5148\n",
      "Training loss at step 325: 0.7313\n",
      "Training loss at step 350: 0.4500\n",
      "Training accuracy: 0.7915 Validation accuracy: 0.7304 Time taken: 122.74s\n",
      "Epoch 40/200\n",
      "Training loss at step 0: 0.5163\n",
      "Training loss at step 25: 0.5115\n",
      "Training loss at step 50: 0.6619\n",
      "Training loss at step 75: 0.5726\n",
      "Training loss at step 100: 0.7124\n",
      "Training loss at step 125: 0.5843\n",
      "Training loss at step 150: 0.6362\n",
      "Training loss at step 175: 0.5113\n",
      "Training loss at step 200: 0.6702\n",
      "Training loss at step 225: 0.6055\n",
      "Training loss at step 250: 0.6841\n",
      "Training loss at step 275: 0.6540\n",
      "Training loss at step 300: 0.5385\n",
      "Training loss at step 325: 0.6305\n",
      "Training loss at step 350: 0.5904\n",
      "Training accuracy: 0.7956 Validation accuracy: 0.7814 Time taken: 123.09s\n",
      "Epoch 41/200\n",
      "Training loss at step 0: 0.7379\n",
      "Training loss at step 25: 0.6299\n",
      "Training loss at step 50: 0.7280\n",
      "Training loss at step 75: 0.6068\n",
      "Training loss at step 100: 0.7226\n",
      "Training loss at step 125: 0.5634\n",
      "Training loss at step 150: 0.5341\n",
      "Training loss at step 175: 0.5023\n",
      "Training loss at step 200: 0.5228\n",
      "Training loss at step 225: 0.7287\n",
      "Training loss at step 250: 0.5663\n",
      "Training loss at step 275: 0.4442\n",
      "Training loss at step 300: 0.5366\n",
      "Training loss at step 325: 0.6567\n",
      "Training loss at step 350: 0.6341\n",
      "Training accuracy: 0.7991 Validation accuracy: 0.7666 Time taken: 123.02s\n",
      "Epoch 42/200\n",
      "Training loss at step 0: 0.5303\n",
      "Training loss at step 25: 0.5688\n",
      "Training loss at step 50: 0.5127\n",
      "Training loss at step 75: 0.5028\n",
      "Training loss at step 100: 0.4629\n",
      "Training loss at step 125: 0.5458\n",
      "Training loss at step 150: 0.5641\n",
      "Training loss at step 175: 0.6075\n",
      "Training loss at step 200: 0.6920\n",
      "Training loss at step 225: 0.6528\n",
      "Training loss at step 250: 0.6031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 275: 0.5508\n",
      "Training loss at step 300: 0.5906\n",
      "Training loss at step 325: 0.5285\n",
      "Training loss at step 350: 0.4799\n",
      "Training accuracy: 0.7974 Validation accuracy: 0.7900 Time taken: 123.39s\n",
      "Epoch 43/200\n",
      "Training loss at step 0: 0.5792\n",
      "Training loss at step 25: 0.5780\n",
      "Training loss at step 50: 0.5603\n",
      "Training loss at step 75: 0.5485\n",
      "Training loss at step 100: 0.5375\n",
      "Training loss at step 125: 0.5798\n",
      "Training loss at step 150: 0.7403\n",
      "Training loss at step 175: 0.6593\n",
      "Training loss at step 200: 0.6523\n",
      "Training loss at step 225: 0.6182\n",
      "Training loss at step 250: 0.7151\n",
      "Training loss at step 275: 0.6157\n",
      "Training loss at step 300: 0.5556\n",
      "Training loss at step 325: 0.5970\n",
      "Training loss at step 350: 0.5957\n",
      "Training accuracy: 0.8059 Validation accuracy: 0.5242 Time taken: 122.84s\n",
      "Epoch 44/200\n",
      "Training loss at step 0: 0.7988\n",
      "Training loss at step 25: 0.4398\n",
      "Training loss at step 50: 0.7304\n",
      "Training loss at step 75: 0.5070\n",
      "Training loss at step 100: 0.4193\n",
      "Training loss at step 125: 0.3736\n",
      "Training loss at step 150: 0.6104\n",
      "Training loss at step 175: 0.6756\n",
      "Training loss at step 200: 0.5087\n",
      "Training loss at step 225: 0.5417\n",
      "Training loss at step 250: 0.5670\n",
      "Training loss at step 275: 0.5223\n",
      "Training loss at step 300: 0.6258\n",
      "Training loss at step 325: 0.3907\n",
      "Training loss at step 350: 0.4921\n",
      "Training accuracy: 0.8054 Validation accuracy: 0.7854 Time taken: 123.28s\n",
      "Epoch 45/200\n",
      "Training loss at step 0: 0.5419\n",
      "Training loss at step 25: 0.3644\n",
      "Training loss at step 50: 0.4010\n",
      "Training loss at step 75: 0.4982\n",
      "Training loss at step 100: 0.7254\n",
      "Training loss at step 125: 0.6411\n",
      "Training loss at step 150: 0.5437\n",
      "Training loss at step 175: 0.6026\n",
      "Training loss at step 200: 0.5615\n",
      "Training loss at step 225: 0.5527\n",
      "Training loss at step 250: 0.7538\n",
      "Training loss at step 275: 0.5921\n",
      "Training loss at step 300: 0.4967\n",
      "Training loss at step 325: 0.5650\n",
      "Training loss at step 350: 0.4865\n",
      "Training accuracy: 0.8082 Validation accuracy: 0.7864 Time taken: 123.04s\n",
      "Epoch 46/200\n",
      "Training loss at step 0: 0.5189\n",
      "Training loss at step 25: 0.4007\n",
      "Training loss at step 50: 0.5185\n",
      "Training loss at step 75: 0.6118\n",
      "Training loss at step 100: 0.3725\n",
      "Training loss at step 125: 0.4252\n",
      "Training loss at step 150: 0.4864\n",
      "Training loss at step 175: 0.4923\n",
      "Training loss at step 200: 0.4923\n",
      "Training loss at step 225: 0.5735\n",
      "Training loss at step 250: 0.4970\n",
      "Training loss at step 275: 0.5189\n",
      "Training loss at step 300: 0.6059\n",
      "Training loss at step 325: 0.6581\n",
      "Training loss at step 350: 0.5336\n",
      "Training accuracy: 0.8118 Validation accuracy: 0.8272 Time taken: 123.16s\n",
      "Epoch 47/200\n",
      "Training loss at step 0: 0.4641\n",
      "Training loss at step 25: 0.5061\n",
      "Training loss at step 50: 0.6950\n",
      "Training loss at step 75: 0.5510\n",
      "Training loss at step 100: 0.3784\n",
      "Training loss at step 125: 0.4743\n",
      "Training loss at step 150: 0.6150\n",
      "Training loss at step 175: 0.6434\n",
      "Training loss at step 200: 0.6652\n",
      "Training loss at step 225: 0.3926\n",
      "Training loss at step 250: 0.4971\n",
      "Training loss at step 275: 0.4095\n",
      "Training loss at step 300: 0.4173\n",
      "Training loss at step 325: 0.5171\n",
      "Training loss at step 350: 0.5597\n",
      "Training accuracy: 0.8144 Validation accuracy: 0.8222 Time taken: 122.71s\n",
      "Epoch 48/200\n",
      "Training loss at step 0: 0.5519\n",
      "Training loss at step 25: 0.4524\n",
      "Training loss at step 50: 0.4964\n",
      "Training loss at step 75: 0.5167\n",
      "Training loss at step 100: 0.5584\n",
      "Training loss at step 125: 0.4787\n",
      "Training loss at step 150: 0.4819\n",
      "Training loss at step 175: 0.3774\n",
      "Training loss at step 200: 0.5050\n",
      "Training loss at step 225: 0.5175\n",
      "Training loss at step 250: 0.4396\n",
      "Training loss at step 275: 0.5034\n",
      "Training loss at step 300: 0.4946\n",
      "Training loss at step 325: 0.6374\n",
      "Training loss at step 350: 0.5171\n",
      "Training accuracy: 0.8174 Validation accuracy: 0.8012 Time taken: 123.22s\n",
      "Epoch 49/200\n",
      "Training loss at step 0: 0.5405\n",
      "Training loss at step 25: 0.5386\n",
      "Training loss at step 50: 0.4995\n",
      "Training loss at step 75: 0.5288\n",
      "Training loss at step 100: 0.3965\n",
      "Training loss at step 125: 0.5030\n",
      "Training loss at step 150: 0.4716\n",
      "Training loss at step 175: 0.4942\n",
      "Training loss at step 200: 0.6170\n",
      "Training loss at step 225: 0.5343\n",
      "Training loss at step 250: 0.5135\n",
      "Training loss at step 275: 0.4580\n",
      "Training loss at step 300: 0.6230\n",
      "Training loss at step 325: 0.6292\n",
      "Training loss at step 350: 0.4306\n",
      "Training accuracy: 0.8192 Validation accuracy: 0.7108 Time taken: 123.08s\n",
      "Epoch 50/200\n",
      "Training loss at step 0: 0.6180\n",
      "Training loss at step 25: 0.4667\n",
      "Training loss at step 50: 0.4868\n",
      "Training loss at step 75: 0.6829\n",
      "Training loss at step 100: 0.4153\n",
      "Training loss at step 125: 0.5790\n",
      "Training loss at step 150: 0.4918\n",
      "Training loss at step 175: 0.4774\n",
      "Training loss at step 200: 0.5020\n",
      "Training loss at step 225: 0.5637\n",
      "Training loss at step 250: 0.4403\n",
      "Training loss at step 275: 0.6780\n",
      "Training loss at step 300: 0.5207\n",
      "Training loss at step 325: 0.5567\n",
      "Training loss at step 350: 0.6549\n",
      "Training accuracy: 0.8209 Validation accuracy: 0.8002 Time taken: 123.39s\n",
      "Epoch 51/200\n",
      "Training loss at step 0: 0.5172\n",
      "Training loss at step 25: 0.4336\n",
      "Training loss at step 50: 0.6145\n",
      "Training loss at step 75: 0.7204\n",
      "Training loss at step 100: 0.4289\n",
      "Training loss at step 125: 0.4740\n",
      "Training loss at step 150: 0.5488\n",
      "Training loss at step 175: 0.3989\n",
      "Training loss at step 200: 0.5402\n",
      "Training loss at step 225: 0.4407\n",
      "Training loss at step 250: 0.4851\n",
      "Training loss at step 275: 0.5214\n",
      "Training loss at step 300: 0.5899\n",
      "Training loss at step 325: 0.4732\n",
      "Training loss at step 350: 0.4839\n",
      "Training accuracy: 0.8240 Validation accuracy: 0.8138 Time taken: 123.32s\n",
      "Epoch 52/200\n",
      "Training loss at step 0: 0.4288\n",
      "Training loss at step 25: 0.6683\n",
      "Training loss at step 50: 0.5513\n",
      "Training loss at step 75: 0.5176\n",
      "Training loss at step 100: 0.4921\n",
      "Training loss at step 125: 0.4790\n",
      "Training loss at step 150: 0.4521\n",
      "Training loss at step 175: 0.4418\n",
      "Training loss at step 200: 0.6238\n",
      "Training loss at step 225: 0.5000\n",
      "Training loss at step 250: 0.3915\n",
      "Training loss at step 275: 0.3689\n",
      "Training loss at step 300: 0.4886\n",
      "Training loss at step 325: 0.4124\n",
      "Training loss at step 350: 0.4133\n",
      "Training accuracy: 0.8274 Validation accuracy: 0.8174 Time taken: 123.53s\n",
      "Epoch 53/200\n",
      "Training loss at step 0: 0.4531\n",
      "Training loss at step 25: 0.5470\n",
      "Training loss at step 50: 0.4421\n",
      "Training loss at step 75: 0.4920\n",
      "Training loss at step 100: 0.5896\n",
      "Training loss at step 125: 0.4854\n",
      "Training loss at step 150: 0.4213\n",
      "Training loss at step 175: 0.5326\n",
      "Training loss at step 200: 0.5692\n",
      "Training loss at step 225: 0.4001\n",
      "Training loss at step 250: 0.3186\n",
      "Training loss at step 275: 0.4951\n",
      "Training loss at step 300: 0.5319\n",
      "Training loss at step 325: 0.5085\n",
      "Training loss at step 350: 0.4776\n",
      "Training accuracy: 0.8296 Validation accuracy: 0.7910 Time taken: 123.06s\n",
      "Epoch 54/200\n",
      "Training loss at step 0: 0.4989\n",
      "Training loss at step 25: 0.3768\n",
      "Training loss at step 50: 0.5283\n",
      "Training loss at step 75: 0.5466\n",
      "Training loss at step 100: 0.5220\n",
      "Training loss at step 125: 0.4058\n",
      "Training loss at step 150: 0.5572\n",
      "Training loss at step 175: 0.5359\n",
      "Training loss at step 200: 0.5040\n",
      "Training loss at step 225: 0.4895\n",
      "Training loss at step 250: 0.3653\n",
      "Training loss at step 275: 0.5065\n",
      "Training loss at step 300: 0.4751\n",
      "Training loss at step 325: 0.4996\n",
      "Training loss at step 350: 0.4130\n",
      "Training accuracy: 0.8249 Validation accuracy: 0.7634 Time taken: 122.50s\n",
      "Epoch 55/200\n",
      "Training loss at step 0: 0.4069\n",
      "Training loss at step 25: 0.4630\n",
      "Training loss at step 50: 0.3896\n",
      "Training loss at step 75: 0.4481\n",
      "Training loss at step 100: 0.6589\n",
      "Training loss at step 125: 0.5714\n",
      "Training loss at step 150: 0.4372\n",
      "Training loss at step 175: 0.4077\n",
      "Training loss at step 200: 0.4419\n",
      "Training loss at step 225: 0.3901\n",
      "Training loss at step 250: 0.3973\n",
      "Training loss at step 275: 0.5501\n",
      "Training loss at step 300: 0.4906\n",
      "Training loss at step 325: 0.5562\n",
      "Training loss at step 350: 0.3917\n",
      "Training accuracy: 0.8281 Validation accuracy: 0.8264 Time taken: 123.79s\n",
      "Epoch 56/200\n",
      "Training loss at step 0: 0.3668\n",
      "Training loss at step 25: 0.5067\n",
      "Training loss at step 50: 0.5395\n",
      "Training loss at step 75: 0.4215\n",
      "Training loss at step 100: 0.4134\n",
      "Training loss at step 125: 0.5178\n",
      "Training loss at step 150: 0.4417\n",
      "Training loss at step 175: 0.4870\n",
      "Training loss at step 200: 0.5147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 225: 0.7230\n",
      "Training loss at step 250: 0.4614\n",
      "Training loss at step 275: 0.5072\n",
      "Training loss at step 300: 0.4770\n",
      "Training loss at step 325: 0.4936\n",
      "Training loss at step 350: 0.4490\n",
      "Training accuracy: 0.8315 Validation accuracy: 0.7700 Time taken: 123.83s\n",
      "Epoch 57/200\n",
      "Training loss at step 0: 0.4492\n",
      "Training loss at step 25: 0.4095\n",
      "Training loss at step 50: 0.4438\n",
      "Training loss at step 75: 0.4267\n",
      "Training loss at step 100: 0.4152\n",
      "Training loss at step 125: 0.6942\n",
      "Training loss at step 150: 0.5223\n",
      "Training loss at step 175: 0.4290\n",
      "Training loss at step 200: 0.5796\n",
      "Training loss at step 225: 0.5565\n",
      "Training loss at step 250: 0.4924\n",
      "Training loss at step 275: 0.3609\n",
      "Training loss at step 300: 0.3185\n",
      "Training loss at step 325: 0.5352\n",
      "Training loss at step 350: 0.5156\n",
      "Training accuracy: 0.8321 Validation accuracy: 0.8448 Time taken: 123.90s\n",
      "Epoch 58/200\n",
      "Training loss at step 0: 0.4409\n",
      "Training loss at step 25: 0.3972\n",
      "Training loss at step 50: 0.3276\n",
      "Training loss at step 75: 0.3790\n",
      "Training loss at step 100: 0.5000\n",
      "Training loss at step 125: 0.5366\n",
      "Training loss at step 150: 0.4163\n",
      "Training loss at step 175: 0.6618\n",
      "Training loss at step 200: 0.4120\n",
      "Training loss at step 225: 0.4231\n",
      "Training loss at step 250: 0.4589\n",
      "Training loss at step 275: 0.4974\n",
      "Training loss at step 300: 0.4569\n",
      "Training loss at step 325: 0.5812\n",
      "Training loss at step 350: 0.5218\n",
      "Training accuracy: 0.8332 Validation accuracy: 0.8204 Time taken: 123.53s\n",
      "Epoch 59/200\n",
      "Training loss at step 0: 0.5048\n",
      "Training loss at step 25: 0.4105\n",
      "Training loss at step 50: 0.6400\n",
      "Training loss at step 75: 0.4115\n",
      "Training loss at step 100: 0.5194\n",
      "Training loss at step 125: 0.5134\n",
      "Training loss at step 150: 0.4474\n",
      "Training loss at step 175: 0.3599\n",
      "Training loss at step 200: 0.4862\n",
      "Training loss at step 225: 0.4869\n",
      "Training loss at step 250: 0.6001\n",
      "Training loss at step 275: 0.4207\n",
      "Training loss at step 300: 0.6071\n",
      "Training loss at step 325: 0.5068\n",
      "Training loss at step 350: 0.5869\n",
      "Training accuracy: 0.8382 Validation accuracy: 0.8268 Time taken: 123.66s\n",
      "Epoch 60/200\n",
      "Training loss at step 0: 0.4537\n",
      "Training loss at step 25: 0.5005\n",
      "Training loss at step 50: 0.4933\n",
      "Training loss at step 75: 0.4381\n",
      "Training loss at step 100: 0.4763\n",
      "Training loss at step 125: 0.4692\n",
      "Training loss at step 150: 0.4672\n",
      "Training loss at step 175: 0.5217\n",
      "Training loss at step 200: 0.4190\n",
      "Training loss at step 225: 0.3702\n",
      "Training loss at step 250: 0.4208\n",
      "Training loss at step 275: 0.5539\n",
      "Training loss at step 300: 0.4709\n",
      "Training loss at step 325: 0.2779\n",
      "Training loss at step 350: 0.5182\n",
      "Training accuracy: 0.8369 Validation accuracy: 0.8112 Time taken: 123.92s\n",
      "Epoch 61/200\n",
      "Training loss at step 0: 0.4570\n",
      "Training loss at step 25: 0.4893\n",
      "Training loss at step 50: 0.5010\n",
      "Training loss at step 75: 0.3000\n",
      "Training loss at step 100: 0.5060\n",
      "Training loss at step 125: 0.4243\n",
      "Training loss at step 150: 0.3333\n",
      "Training loss at step 175: 0.4428\n",
      "Training loss at step 200: 0.5448\n",
      "Training loss at step 225: 0.3315\n",
      "Training loss at step 250: 0.5476\n",
      "Training loss at step 275: 0.2832\n",
      "Training loss at step 300: 0.4202\n",
      "Training loss at step 325: 0.5370\n",
      "Training loss at step 350: 0.3863\n",
      "Training accuracy: 0.8410 Validation accuracy: 0.8526 Time taken: 123.65s\n",
      "Epoch 62/200\n",
      "Training loss at step 0: 0.3904\n",
      "Training loss at step 25: 0.5574\n",
      "Training loss at step 50: 0.5943\n",
      "Training loss at step 75: 0.3831\n",
      "Training loss at step 100: 0.5078\n",
      "Training loss at step 125: 0.4181\n",
      "Training loss at step 150: 0.4812\n",
      "Training loss at step 175: 0.4144\n",
      "Training loss at step 200: 0.4066\n",
      "Training loss at step 225: 0.4794\n",
      "Training loss at step 250: 0.4642\n",
      "Training loss at step 275: 0.5900\n",
      "Training loss at step 300: 0.5429\n",
      "Training loss at step 325: 0.4057\n",
      "Training loss at step 350: 0.4010\n",
      "Training accuracy: 0.8410 Validation accuracy: 0.8226 Time taken: 123.43s\n",
      "Epoch 63/200\n",
      "Training loss at step 0: 0.5350\n",
      "Training loss at step 25: 0.4311\n",
      "Training loss at step 50: 0.3901\n",
      "Training loss at step 75: 0.3329\n",
      "Training loss at step 100: 0.2804\n",
      "Training loss at step 125: 0.3907\n",
      "Training loss at step 150: 0.3746\n",
      "Training loss at step 175: 0.4572\n",
      "Training loss at step 200: 0.5052\n",
      "Training loss at step 225: 0.4796\n",
      "Training loss at step 250: 0.5039\n",
      "Training loss at step 275: 0.3560\n",
      "Training loss at step 300: 0.5618\n",
      "Training loss at step 325: 0.4677\n",
      "Training loss at step 350: 0.4833\n",
      "Training accuracy: 0.8478 Validation accuracy: 0.8086 Time taken: 123.87s\n",
      "Epoch 64/200\n",
      "Training loss at step 0: 0.4705\n",
      "Training loss at step 25: 0.3709\n",
      "Training loss at step 50: 0.4115\n",
      "Training loss at step 75: 0.4342\n",
      "Training loss at step 100: 0.4154\n",
      "Training loss at step 125: 0.3915\n",
      "Training loss at step 150: 0.4830\n",
      "Training loss at step 175: 0.6026\n",
      "Training loss at step 200: 0.4215\n",
      "Training loss at step 225: 0.4722\n",
      "Training loss at step 250: 0.2826\n",
      "Training loss at step 275: 0.5258\n",
      "Training loss at step 300: 0.3859\n",
      "Training loss at step 325: 0.5604\n",
      "Training loss at step 350: 0.3865\n",
      "Training accuracy: 0.8465 Validation accuracy: 0.8070 Time taken: 123.15s\n",
      "Epoch 65/200\n",
      "Training loss at step 0: 0.4308\n",
      "Training loss at step 25: 0.3252\n",
      "Training loss at step 50: 0.3739\n",
      "Training loss at step 75: 0.4691\n",
      "Training loss at step 100: 0.3183\n",
      "Training loss at step 125: 0.5294\n",
      "Training loss at step 150: 0.4108\n",
      "Training loss at step 175: 0.4070\n",
      "Training loss at step 200: 0.3403\n",
      "Training loss at step 225: 0.6212\n",
      "Training loss at step 250: 0.2461\n",
      "Training loss at step 275: 0.5003\n",
      "Training loss at step 300: 0.4924\n",
      "Training loss at step 325: 0.3870\n",
      "Training loss at step 350: 0.4161\n",
      "Training accuracy: 0.8470 Validation accuracy: 0.8228 Time taken: 123.11s\n",
      "Epoch 66/200\n",
      "Training loss at step 0: 0.4881\n",
      "Training loss at step 25: 0.4784\n",
      "Training loss at step 50: 0.4317\n",
      "Training loss at step 75: 0.4833\n",
      "Training loss at step 100: 0.3540\n",
      "Training loss at step 125: 0.5135\n",
      "Training loss at step 150: 0.4091\n",
      "Training loss at step 175: 0.5394\n",
      "Training loss at step 200: 0.4172\n",
      "Training loss at step 225: 0.4300\n",
      "Training loss at step 250: 0.4112\n",
      "Training loss at step 275: 0.4597\n",
      "Training loss at step 300: 0.6208\n",
      "Training loss at step 325: 0.3615\n",
      "Training loss at step 350: 0.4779\n",
      "Training accuracy: 0.8493 Validation accuracy: 0.8370 Time taken: 124.06s\n",
      "Epoch 67/200\n",
      "Training loss at step 0: 0.5761\n",
      "Training loss at step 25: 0.4435\n",
      "Training loss at step 50: 0.5856\n",
      "Training loss at step 75: 0.4110\n",
      "Training loss at step 100: 0.4827\n",
      "Training loss at step 125: 0.3714\n",
      "Training loss at step 150: 0.4871\n",
      "Training loss at step 175: 0.4986\n",
      "Training loss at step 200: 0.4081\n",
      "Training loss at step 225: 0.2570\n",
      "Training loss at step 250: 0.3968\n",
      "Training loss at step 275: 0.4436\n",
      "Training loss at step 300: 0.3955\n",
      "Training loss at step 325: 0.3914\n",
      "Training loss at step 350: 0.3988\n",
      "Training accuracy: 0.8486 Validation accuracy: 0.7664 Time taken: 123.37s\n",
      "Epoch 68/200\n",
      "Training loss at step 0: 0.4789\n",
      "Training loss at step 25: 0.4287\n",
      "Training loss at step 50: 0.4005\n",
      "Training loss at step 75: 0.5349\n",
      "Training loss at step 100: 0.3622\n",
      "Training loss at step 125: 0.4580\n",
      "Training loss at step 150: 0.4657\n",
      "Training loss at step 175: 0.4385\n",
      "Training loss at step 200: 0.5456\n",
      "Training loss at step 225: 0.3808\n",
      "Training loss at step 250: 0.4662\n",
      "Training loss at step 275: 0.3242\n",
      "Training loss at step 300: 0.5053\n",
      "Training loss at step 325: 0.4207\n",
      "Training loss at step 350: 0.4108\n",
      "Training accuracy: 0.8512 Validation accuracy: 0.7432 Time taken: 123.47s\n",
      "Epoch 69/200\n",
      "Training loss at step 0: 0.4266\n",
      "Training loss at step 25: 0.4492\n",
      "Training loss at step 50: 0.4434\n",
      "Training loss at step 75: 0.2413\n",
      "Training loss at step 100: 0.5387\n",
      "Training loss at step 125: 0.3133\n",
      "Training loss at step 150: 0.3699\n",
      "Training loss at step 175: 0.4022\n",
      "Training loss at step 200: 0.3764\n",
      "Training loss at step 225: 0.4521\n",
      "Training loss at step 250: 0.4471\n",
      "Training loss at step 275: 0.4857\n",
      "Training loss at step 300: 0.4887\n",
      "Training loss at step 325: 0.3678\n",
      "Training loss at step 350: 0.3268\n",
      "Training accuracy: 0.8531 Validation accuracy: 0.8166 Time taken: 124.32s\n",
      "Epoch 70/200\n",
      "Training loss at step 0: 0.5012\n",
      "Training loss at step 25: 0.3330\n",
      "Training loss at step 50: 0.3599\n",
      "Training loss at step 75: 0.4325\n",
      "Training loss at step 100: 0.4012\n",
      "Training loss at step 125: 0.5250\n",
      "Training loss at step 150: 0.4304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 175: 0.5481\n",
      "Training loss at step 200: 0.3591\n",
      "Training loss at step 225: 0.3113\n",
      "Training loss at step 250: 0.4353\n",
      "Training loss at step 275: 0.3363\n",
      "Training loss at step 300: 0.3930\n",
      "Training loss at step 325: 0.4137\n",
      "Training loss at step 350: 0.4903\n",
      "Training accuracy: 0.8544 Validation accuracy: 0.8160 Time taken: 123.59s\n",
      "Epoch 71/200\n",
      "Training loss at step 0: 0.3306\n",
      "Training loss at step 25: 0.4765\n",
      "Training loss at step 50: 0.2773\n",
      "Training loss at step 75: 0.4236\n",
      "Training loss at step 100: 0.4184\n",
      "Training loss at step 125: 0.3682\n",
      "Training loss at step 150: 0.4277\n",
      "Training loss at step 175: 0.4749\n",
      "Training loss at step 200: 0.5512\n",
      "Training loss at step 225: 0.4058\n",
      "Training loss at step 250: 0.3070\n",
      "Training loss at step 275: 0.4021\n",
      "Training loss at step 300: 0.3555\n",
      "Training loss at step 325: 0.4198\n",
      "Training loss at step 350: 0.3219\n",
      "Training accuracy: 0.8582 Validation accuracy: 0.8618 Time taken: 123.94s\n",
      "Epoch 72/200\n",
      "Training loss at step 0: 0.2569\n",
      "Training loss at step 25: 0.3126\n",
      "Training loss at step 50: 0.4064\n",
      "Training loss at step 75: 0.3717\n",
      "Training loss at step 100: 0.4945\n",
      "Training loss at step 125: 0.4584\n",
      "Training loss at step 150: 0.3232\n",
      "Training loss at step 175: 0.4930\n",
      "Training loss at step 200: 0.4397\n",
      "Training loss at step 225: 0.2889\n",
      "Training loss at step 250: 0.4264\n",
      "Training loss at step 275: 0.5452\n",
      "Training loss at step 300: 0.3386\n",
      "Training loss at step 325: 0.5082\n",
      "Training loss at step 350: 0.3364\n",
      "Training accuracy: 0.8569 Validation accuracy: 0.8296 Time taken: 123.38s\n",
      "Epoch 73/200\n",
      "Training loss at step 0: 0.3242\n",
      "Training loss at step 25: 0.4526\n",
      "Training loss at step 50: 0.4856\n",
      "Training loss at step 75: 0.4194\n",
      "Training loss at step 100: 0.3440\n",
      "Training loss at step 125: 0.5075\n",
      "Training loss at step 150: 0.5551\n",
      "Training loss at step 175: 0.3268\n",
      "Training loss at step 200: 0.2823\n",
      "Training loss at step 225: 0.3183\n",
      "Training loss at step 250: 0.4989\n",
      "Training loss at step 275: 0.3657\n",
      "Training loss at step 300: 0.3040\n",
      "Training loss at step 325: 0.5319\n",
      "Training loss at step 350: 0.3792\n",
      "Training accuracy: 0.8615 Validation accuracy: 0.8488 Time taken: 123.76s\n",
      "Epoch 74/200\n",
      "Training loss at step 0: 0.3287\n",
      "Training loss at step 25: 0.3217\n",
      "Training loss at step 50: 0.3313\n",
      "Training loss at step 75: 0.2583\n",
      "Training loss at step 100: 0.3644\n",
      "Training loss at step 125: 0.3256\n",
      "Training loss at step 150: 0.2957\n",
      "Training loss at step 175: 0.4247\n",
      "Training loss at step 200: 0.2244\n",
      "Training loss at step 225: 0.3902\n",
      "Training loss at step 250: 0.3284\n",
      "Training loss at step 275: 0.3499\n",
      "Training loss at step 300: 0.3539\n",
      "Training loss at step 325: 0.4113\n",
      "Training loss at step 350: 0.3873\n",
      "Training accuracy: 0.8589 Validation accuracy: 0.8282 Time taken: 123.09s\n",
      "Epoch 75/200\n",
      "Training loss at step 0: 0.3006\n",
      "Training loss at step 25: 0.3152\n",
      "Training loss at step 50: 0.4823\n",
      "Training loss at step 75: 0.3377\n",
      "Training loss at step 100: 0.4474\n",
      "Training loss at step 125: 0.4976\n",
      "Training loss at step 150: 0.3247\n",
      "Training loss at step 175: 0.3278\n",
      "Training loss at step 200: 0.2599\n",
      "Training loss at step 225: 0.4371\n",
      "Training loss at step 250: 0.3825\n",
      "Training loss at step 275: 0.3021\n",
      "Training loss at step 300: 0.4950\n",
      "Training loss at step 325: 0.3129\n",
      "Training loss at step 350: 0.3870\n",
      "Training accuracy: 0.8604 Validation accuracy: 0.7978 Time taken: 123.51s\n",
      "Epoch 76/200\n",
      "Training loss at step 0: 0.4200\n",
      "Training loss at step 25: 0.3653\n",
      "Training loss at step 50: 0.3296\n",
      "Training loss at step 75: 0.3933\n",
      "Training loss at step 100: 0.3965\n",
      "Training loss at step 125: 0.2848\n",
      "Training loss at step 150: 0.2602\n",
      "Training loss at step 175: 0.3745\n",
      "Training loss at step 200: 0.2966\n",
      "Training loss at step 225: 0.5304\n",
      "Training loss at step 250: 0.3350\n",
      "Training loss at step 275: 0.2547\n",
      "Training loss at step 300: 0.4451\n",
      "Training loss at step 325: 0.3752\n",
      "Training loss at step 350: 0.4891\n",
      "Training accuracy: 0.8632 Validation accuracy: 0.8436 Time taken: 123.78s\n",
      "Epoch 77/200\n",
      "Training loss at step 0: 0.4096\n",
      "Training loss at step 25: 0.3051\n",
      "Training loss at step 50: 0.4206\n",
      "Training loss at step 75: 0.2964\n",
      "Training loss at step 100: 0.2420\n",
      "Training loss at step 125: 0.2761\n",
      "Training loss at step 150: 0.3604\n",
      "Training loss at step 175: 0.4101\n",
      "Training loss at step 200: 0.3574\n",
      "Training loss at step 225: 0.3597\n",
      "Training loss at step 250: 0.2915\n",
      "Training loss at step 275: 0.3671\n",
      "Training loss at step 300: 0.3999\n",
      "Training loss at step 325: 0.3717\n",
      "Training loss at step 350: 0.3831\n",
      "Training accuracy: 0.8661 Validation accuracy: 0.8020 Time taken: 123.38s\n",
      "Epoch 78/200\n",
      "Training loss at step 0: 0.3434\n",
      "Training loss at step 25: 0.3845\n",
      "Training loss at step 50: 0.3822\n",
      "Training loss at step 75: 0.4215\n",
      "Training loss at step 100: 0.4207\n",
      "Training loss at step 125: 0.4270\n",
      "Training loss at step 150: 0.3231\n",
      "Training loss at step 175: 0.5184\n",
      "Training loss at step 200: 0.3977\n",
      "Training loss at step 225: 0.3410\n",
      "Training loss at step 250: 0.5192\n",
      "Training loss at step 275: 0.3865\n",
      "Training loss at step 300: 0.4010\n",
      "Training loss at step 325: 0.3555\n",
      "Training loss at step 350: 0.3030\n",
      "Training accuracy: 0.8641 Validation accuracy: 0.8688 Time taken: 123.17s\n",
      "Epoch 79/200\n",
      "Training loss at step 0: 0.2880\n",
      "Training loss at step 25: 0.3827\n",
      "Training loss at step 50: 0.3215\n",
      "Training loss at step 75: 0.2922\n",
      "Training loss at step 100: 0.3324\n",
      "Training loss at step 125: 0.4059\n",
      "Training loss at step 150: 0.2640\n",
      "Training loss at step 175: 0.3733\n",
      "Training loss at step 200: 0.4509\n",
      "Training loss at step 225: 0.3806\n",
      "Training loss at step 250: 0.3073\n",
      "Training loss at step 275: 0.3476\n",
      "Training loss at step 300: 0.3033\n",
      "Training loss at step 325: 0.4133\n",
      "Training loss at step 350: 0.4065\n",
      "Training accuracy: 0.8680 Validation accuracy: 0.8110 Time taken: 123.45s\n",
      "Epoch 80/200\n",
      "Training loss at step 0: 0.3462\n",
      "Training loss at step 25: 0.3640\n",
      "Training loss at step 50: 0.3633\n",
      "Training loss at step 75: 0.3029\n",
      "Training loss at step 100: 0.3665\n",
      "Training loss at step 125: 0.3270\n",
      "Training loss at step 150: 0.4475\n",
      "Training loss at step 175: 0.3097\n",
      "Training loss at step 200: 0.4397\n",
      "Training loss at step 225: 0.3993\n",
      "Training loss at step 250: 0.4370\n",
      "Training loss at step 275: 0.3966\n",
      "Training loss at step 300: 0.3813\n",
      "Training loss at step 325: 0.4097\n",
      "Training loss at step 350: 0.3221\n",
      "Training accuracy: 0.8670 Validation accuracy: 0.8310 Time taken: 122.97s\n",
      "Epoch 81/200\n",
      "Training loss at step 0: 0.4147\n",
      "Training loss at step 25: 0.4814\n",
      "Training loss at step 50: 0.2787\n",
      "Training loss at step 75: 0.3486\n",
      "Training loss at step 100: 0.3978\n",
      "Training loss at step 125: 0.4473\n",
      "Training loss at step 150: 0.3741\n",
      "Training loss at step 175: 0.3395\n",
      "Training loss at step 200: 0.4337\n",
      "Training loss at step 225: 0.4157\n",
      "Training loss at step 250: 0.4302\n",
      "Training loss at step 275: 0.4281\n",
      "Training loss at step 300: 0.3505\n",
      "Training loss at step 325: 0.3974\n",
      "Training loss at step 350: 0.3557\n",
      "Training accuracy: 0.8666 Validation accuracy: 0.8634 Time taken: 123.40s\n",
      "Epoch 82/200\n",
      "Training loss at step 0: 0.3042\n",
      "Training loss at step 25: 0.4152\n",
      "Training loss at step 50: 0.3847\n",
      "Training loss at step 75: 0.3506\n",
      "Training loss at step 100: 0.2416\n",
      "Training loss at step 125: 0.2824\n",
      "Training loss at step 150: 0.3230\n",
      "Training loss at step 175: 0.5094\n",
      "Training loss at step 200: 0.3539\n",
      "Training loss at step 225: 0.2262\n",
      "Training loss at step 250: 0.3488\n",
      "Training loss at step 275: 0.3409\n",
      "Training loss at step 300: 0.4411\n",
      "Training loss at step 325: 0.3081\n",
      "Training loss at step 350: 0.3730\n",
      "Training accuracy: 0.8721 Validation accuracy: 0.8218 Time taken: 123.22s\n",
      "Epoch 83/200\n",
      "Training loss at step 0: 0.4505\n",
      "Training loss at step 25: 0.2565\n",
      "Training loss at step 50: 0.3651\n",
      "Training loss at step 75: 0.3512\n",
      "Training loss at step 100: 0.3298\n",
      "Training loss at step 125: 0.3179\n",
      "Training loss at step 150: 0.2404\n",
      "Training loss at step 175: 0.4827\n",
      "Training loss at step 200: 0.2794\n",
      "Training loss at step 225: 0.4382\n",
      "Training loss at step 250: 0.5361\n",
      "Training loss at step 275: 0.3040\n",
      "Training loss at step 300: 0.4666\n",
      "Training loss at step 325: 0.3359\n",
      "Training loss at step 350: 0.2424\n",
      "Training accuracy: 0.8702 Validation accuracy: 0.8128 Time taken: 122.92s\n",
      "Epoch 84/200\n",
      "Training loss at step 0: 0.4535\n",
      "Training loss at step 25: 0.3797\n",
      "Training loss at step 50: 0.2821\n",
      "Training loss at step 75: 0.3895\n",
      "Training loss at step 100: 0.6059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 125: 0.4337\n",
      "Training loss at step 150: 0.3871\n",
      "Training loss at step 175: 0.3432\n",
      "Training loss at step 200: 0.4616\n",
      "Training loss at step 225: 0.4974\n",
      "Training loss at step 250: 0.3433\n",
      "Training loss at step 275: 0.3286\n",
      "Training loss at step 300: 0.4016\n",
      "Training loss at step 325: 0.3037\n",
      "Training loss at step 350: 0.4942\n",
      "Training accuracy: 0.8737 Validation accuracy: 0.8462 Time taken: 123.10s\n",
      "Epoch 85/200\n",
      "Training loss at step 0: 0.3740\n",
      "Training loss at step 25: 0.4232\n",
      "Training loss at step 50: 0.3114\n",
      "Training loss at step 75: 0.3080\n",
      "Training loss at step 100: 0.4020\n",
      "Training loss at step 125: 0.2665\n",
      "Training loss at step 150: 0.2252\n",
      "Training loss at step 175: 0.3029\n",
      "Training loss at step 200: 0.2823\n",
      "Training loss at step 225: 0.4224\n",
      "Training loss at step 250: 0.2370\n",
      "Training loss at step 275: 0.3698\n",
      "Training loss at step 300: 0.3837\n",
      "Training loss at step 325: 0.2771\n",
      "Training loss at step 350: 0.3204\n",
      "Training accuracy: 0.8743 Validation accuracy: 0.8366 Time taken: 122.91s\n",
      "Epoch 86/200\n",
      "Training loss at step 0: 0.3129\n",
      "Training loss at step 25: 0.4325\n",
      "Training loss at step 50: 0.4771\n",
      "Training loss at step 75: 0.3464\n",
      "Training loss at step 100: 0.3843\n",
      "Training loss at step 125: 0.2054\n",
      "Training loss at step 150: 0.3046\n",
      "Training loss at step 175: 0.3817\n",
      "Training loss at step 200: 0.4978\n",
      "Training loss at step 225: 0.3607\n",
      "Training loss at step 250: 0.3433\n",
      "Training loss at step 275: 0.3366\n",
      "Training loss at step 300: 0.3489\n",
      "Training loss at step 325: 0.4258\n",
      "Training loss at step 350: 0.4010\n",
      "Training accuracy: 0.8762 Validation accuracy: 0.8732 Time taken: 122.96s\n",
      "Epoch 87/200\n",
      "Training loss at step 0: 0.2828\n",
      "Training loss at step 25: 0.2947\n",
      "Training loss at step 50: 0.3152\n",
      "Training loss at step 75: 0.4592\n",
      "Training loss at step 100: 0.2933\n",
      "Training loss at step 125: 0.3690\n",
      "Training loss at step 150: 0.2918\n",
      "Training loss at step 175: 0.2952\n",
      "Training loss at step 200: 0.2515\n",
      "Training loss at step 225: 0.3310\n",
      "Training loss at step 250: 0.4091\n",
      "Training loss at step 275: 0.3899\n",
      "Training loss at step 300: 0.3786\n",
      "Training loss at step 325: 0.4286\n",
      "Training loss at step 350: 0.3664\n",
      "Training accuracy: 0.8712 Validation accuracy: 0.8532 Time taken: 123.26s\n",
      "Epoch 88/200\n",
      "Training loss at step 0: 0.2918\n",
      "Training loss at step 25: 0.3833\n",
      "Training loss at step 50: 0.2906\n",
      "Training loss at step 75: 0.3456\n",
      "Training loss at step 100: 0.4418\n",
      "Training loss at step 125: 0.4013\n",
      "Training loss at step 150: 0.4162\n",
      "Training loss at step 175: 0.4011\n",
      "Training loss at step 200: 0.3018\n",
      "Training loss at step 225: 0.2529\n",
      "Training loss at step 250: 0.3627\n",
      "Training loss at step 275: 0.5037\n",
      "Training loss at step 300: 0.4919\n",
      "Training loss at step 325: 0.4136\n",
      "Training loss at step 350: 0.3020\n",
      "Training accuracy: 0.8757 Validation accuracy: 0.8368 Time taken: 123.02s\n",
      "Epoch 89/200\n",
      "Training loss at step 0: 0.2472\n",
      "Training loss at step 25: 0.3053\n",
      "Training loss at step 50: 0.2717\n",
      "Training loss at step 75: 0.4235\n",
      "Training loss at step 100: 0.4341\n",
      "Training loss at step 125: 0.3160\n",
      "Training loss at step 150: 0.2904\n",
      "Training loss at step 175: 0.3714\n",
      "Training loss at step 200: 0.3634\n",
      "Training loss at step 225: 0.4282\n",
      "Training loss at step 250: 0.4327\n",
      "Training loss at step 275: 0.4373\n",
      "Training loss at step 300: 0.3495\n",
      "Training loss at step 325: 0.3918\n",
      "Training loss at step 350: 0.3277\n",
      "Training accuracy: 0.8747 Validation accuracy: 0.8280 Time taken: 122.71s\n",
      "Epoch 90/200\n",
      "Training loss at step 0: 0.4471\n",
      "Training loss at step 25: 0.4213\n",
      "Training loss at step 50: 0.2894\n",
      "Training loss at step 75: 0.5508\n",
      "Training loss at step 100: 0.2783\n",
      "Training loss at step 125: 0.3941\n",
      "Training loss at step 150: 0.2783\n",
      "Training loss at step 175: 0.4071\n",
      "Training loss at step 200: 0.5845\n",
      "Training loss at step 225: 0.3034\n",
      "Training loss at step 250: 0.2716\n",
      "Training loss at step 275: 0.4057\n",
      "Training loss at step 300: 0.3407\n",
      "Training loss at step 325: 0.2731\n",
      "Training loss at step 350: 0.2936\n",
      "Training accuracy: 0.8755 Validation accuracy: 0.8502 Time taken: 123.31s\n",
      "Epoch 91/200\n",
      "Training loss at step 0: 0.2345\n",
      "Training loss at step 25: 0.4031\n",
      "Training loss at step 50: 0.3078\n",
      "Training loss at step 75: 0.3038\n",
      "Training loss at step 100: 0.3133\n",
      "Training loss at step 125: 0.3509\n",
      "Training loss at step 150: 0.2896\n",
      "Training loss at step 175: 0.3721\n",
      "Training loss at step 200: 0.2760\n",
      "Training loss at step 225: 0.2351\n",
      "Training loss at step 250: 0.4340\n",
      "Training loss at step 275: 0.2810\n",
      "Training loss at step 300: 0.2749\n",
      "Training loss at step 325: 0.3834\n",
      "Training loss at step 350: 0.3670\n",
      "Training accuracy: 0.8793 Validation accuracy: 0.8202 Time taken: 124.04s\n",
      "Epoch 92/200\n",
      "Training loss at step 0: 0.3475\n",
      "Training loss at step 25: 0.2852\n",
      "Training loss at step 50: 0.3245\n",
      "Training loss at step 75: 0.3566\n",
      "Training loss at step 100: 0.3557\n",
      "Training loss at step 125: 0.3183\n",
      "Training loss at step 150: 0.3992\n",
      "Training loss at step 175: 0.3224\n",
      "Training loss at step 200: 0.3678\n",
      "Training loss at step 225: 0.4337\n",
      "Training loss at step 250: 0.3123\n",
      "Training loss at step 275: 0.5007\n",
      "Training loss at step 300: 0.4531\n",
      "Training loss at step 325: 0.1942\n",
      "Training loss at step 350: 0.3776\n",
      "Training accuracy: 0.8818 Validation accuracy: 0.8054 Time taken: 122.57s\n",
      "Epoch 93/200\n",
      "Training loss at step 0: 0.4207\n",
      "Training loss at step 25: 0.2994\n",
      "Training loss at step 50: 0.3597\n",
      "Training loss at step 75: 0.3511\n",
      "Training loss at step 100: 0.2723\n",
      "Training loss at step 125: 0.3373\n",
      "Training loss at step 150: 0.2767\n",
      "Training loss at step 175: 0.3176\n",
      "Training loss at step 200: 0.2747\n",
      "Training loss at step 225: 0.3186\n",
      "Training loss at step 250: 0.4087\n",
      "Training loss at step 275: 0.3464\n",
      "Training loss at step 300: 0.2616\n",
      "Training loss at step 325: 0.2315\n",
      "Training loss at step 350: 0.1974\n",
      "Training accuracy: 0.8804 Validation accuracy: 0.8448 Time taken: 122.59s\n",
      "Epoch 94/200\n",
      "Training loss at step 0: 0.4693\n",
      "Training loss at step 25: 0.2565\n",
      "Training loss at step 50: 0.3142\n",
      "Training loss at step 75: 0.3395\n",
      "Training loss at step 100: 0.3523\n",
      "Training loss at step 125: 0.4468\n",
      "Training loss at step 150: 0.3126\n",
      "Training loss at step 175: 0.4199\n",
      "Training loss at step 200: 0.3132\n",
      "Training loss at step 225: 0.4490\n",
      "Training loss at step 250: 0.3755\n",
      "Training loss at step 275: 0.3859\n",
      "Training loss at step 300: 0.4554\n",
      "Training loss at step 325: 0.3836\n",
      "Training loss at step 350: 0.3956\n",
      "Training accuracy: 0.8808 Validation accuracy: 0.8218 Time taken: 123.49s\n",
      "Epoch 95/200\n",
      "Training loss at step 0: 0.2436\n",
      "Training loss at step 25: 0.2112\n",
      "Training loss at step 50: 0.4513\n",
      "Training loss at step 75: 0.4118\n",
      "Training loss at step 100: 0.3115\n",
      "Training loss at step 125: 0.2640\n",
      "Training loss at step 150: 0.3382\n",
      "Training loss at step 175: 0.2764\n",
      "Training loss at step 200: 0.2918\n",
      "Training loss at step 225: 0.3245\n",
      "Training loss at step 250: 0.3281\n",
      "Training loss at step 275: 0.1532\n",
      "Training loss at step 300: 0.2271\n",
      "Training loss at step 325: 0.2903\n",
      "Training loss at step 350: 0.4341\n",
      "Training accuracy: 0.8848 Validation accuracy: 0.8542 Time taken: 122.95s\n",
      "Epoch 96/200\n",
      "Training loss at step 0: 0.2442\n",
      "Training loss at step 25: 0.3466\n",
      "Training loss at step 50: 0.2969\n",
      "Training loss at step 75: 0.3882\n",
      "Training loss at step 100: 0.2908\n",
      "Training loss at step 125: 0.3252\n",
      "Training loss at step 150: 0.2224\n",
      "Training loss at step 175: 0.2527\n",
      "Training loss at step 200: 0.3053\n",
      "Training loss at step 225: 0.3913\n",
      "Training loss at step 250: 0.3665\n",
      "Training loss at step 275: 0.4090\n",
      "Training loss at step 300: 0.3175\n",
      "Training loss at step 325: 0.4341\n",
      "Training loss at step 350: 0.3396\n",
      "Training accuracy: 0.8850 Validation accuracy: 0.7016 Time taken: 122.83s\n",
      "Epoch 97/200\n",
      "Training loss at step 0: 0.5003\n",
      "Training loss at step 25: 0.3851\n",
      "Training loss at step 50: 0.3475\n",
      "Training loss at step 75: 0.2441\n",
      "Training loss at step 100: 0.2673\n",
      "Training loss at step 125: 0.3219\n",
      "Training loss at step 150: 0.3841\n",
      "Training loss at step 175: 0.4132\n",
      "Training loss at step 200: 0.4948\n",
      "Training loss at step 225: 0.3459\n",
      "Training loss at step 250: 0.4295\n",
      "Training loss at step 275: 0.2306\n",
      "Training loss at step 300: 0.3716\n",
      "Training loss at step 325: 0.4855\n",
      "Training loss at step 350: 0.3002\n",
      "Training accuracy: 0.8815 Validation accuracy: 0.7940 Time taken: 122.95s\n",
      "Epoch 98/200\n",
      "Training loss at step 0: 0.3214\n",
      "Training loss at step 25: 0.2757\n",
      "Training loss at step 50: 0.2419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 75: 0.3602\n",
      "Training loss at step 100: 0.3380\n",
      "Training loss at step 125: 0.2852\n",
      "Training loss at step 150: 0.3326\n",
      "Training loss at step 175: 0.3011\n",
      "Training loss at step 200: 0.3567\n",
      "Training loss at step 225: 0.3057\n",
      "Training loss at step 250: 0.3427\n",
      "Training loss at step 275: 0.2839\n",
      "Training loss at step 300: 0.3017\n",
      "Training loss at step 325: 0.3683\n",
      "Training loss at step 350: 0.2665\n",
      "Training accuracy: 0.8822 Validation accuracy: 0.8680 Time taken: 122.53s\n",
      "Epoch 99/200\n",
      "Training loss at step 0: 0.4819\n",
      "Training loss at step 25: 0.2401\n",
      "Training loss at step 50: 0.2118\n",
      "Training loss at step 75: 0.2973\n",
      "Training loss at step 100: 0.2434\n",
      "Training loss at step 125: 0.4667\n",
      "Training loss at step 150: 0.2613\n",
      "Training loss at step 175: 0.2237\n",
      "Training loss at step 200: 0.2818\n",
      "Training loss at step 225: 0.2980\n",
      "Training loss at step 250: 0.2882\n",
      "Training loss at step 275: 0.3754\n",
      "Training loss at step 300: 0.2266\n",
      "Training loss at step 325: 0.3324\n",
      "Training loss at step 350: 0.2372\n",
      "Training accuracy: 0.8867 Validation accuracy: 0.8456 Time taken: 123.26s\n",
      "Epoch 100/200\n",
      "Training loss at step 0: 0.2842\n",
      "Training loss at step 25: 0.3489\n",
      "Training loss at step 50: 0.3284\n",
      "Training loss at step 75: 0.2968\n",
      "Training loss at step 100: 0.2065\n",
      "Training loss at step 125: 0.2969\n",
      "Training loss at step 150: 0.3113\n",
      "Training loss at step 175: 0.3214\n",
      "Training loss at step 200: 0.2829\n",
      "Training loss at step 225: 0.3411\n",
      "Training loss at step 250: 0.3577\n",
      "Training loss at step 275: 0.2616\n",
      "Training loss at step 300: 0.3881\n",
      "Training loss at step 325: 0.2541\n",
      "Training loss at step 350: 0.2079\n",
      "Training accuracy: 0.8893 Validation accuracy: 0.8710 Time taken: 122.74s\n",
      "Epoch 101/200\n",
      "Training loss at step 0: 0.3077\n",
      "Training loss at step 25: 0.3300\n",
      "Training loss at step 50: 0.2720\n",
      "Training loss at step 75: 0.1851\n",
      "Training loss at step 100: 0.2845\n",
      "Training loss at step 125: 0.3239\n",
      "Training loss at step 150: 0.2420\n",
      "Training loss at step 175: 0.2998\n",
      "Training loss at step 200: 0.2053\n",
      "Training loss at step 225: 0.3671\n",
      "Training loss at step 250: 0.2453\n",
      "Training loss at step 275: 0.2583\n",
      "Training loss at step 300: 0.2604\n",
      "Training loss at step 325: 0.2789\n",
      "Training loss at step 350: 0.1932\n",
      "Training accuracy: 0.9016 Validation accuracy: 0.8970 Time taken: 124.38s\n",
      "Epoch 102/200\n",
      "Training loss at step 0: 0.1995\n",
      "Training loss at step 25: 0.2494\n",
      "Training loss at step 50: 0.2113\n",
      "Training loss at step 75: 0.2978\n",
      "Training loss at step 100: 0.2766\n",
      "Training loss at step 125: 0.2321\n",
      "Training loss at step 150: 0.3317\n",
      "Training loss at step 175: 0.3094\n",
      "Training loss at step 200: 0.2190\n",
      "Training loss at step 225: 0.3590\n",
      "Training loss at step 250: 0.2109\n",
      "Training loss at step 275: 0.2500\n",
      "Training loss at step 300: 0.2810\n",
      "Training loss at step 325: 0.2707\n",
      "Training loss at step 350: 0.3691\n",
      "Training accuracy: 0.9090 Validation accuracy: 0.8992 Time taken: 123.91s\n",
      "Epoch 103/200\n",
      "Training loss at step 0: 0.3018\n",
      "Training loss at step 25: 0.3106\n",
      "Training loss at step 50: 0.2887\n",
      "Training loss at step 75: 0.2284\n",
      "Training loss at step 100: 0.2586\n",
      "Training loss at step 125: 0.2861\n",
      "Training loss at step 150: 0.2761\n",
      "Training loss at step 175: 0.2181\n",
      "Training loss at step 200: 0.2611\n",
      "Training loss at step 225: 0.2327\n",
      "Training loss at step 250: 0.2116\n",
      "Training loss at step 275: 0.2539\n",
      "Training loss at step 300: 0.3351\n",
      "Training loss at step 325: 0.2797\n",
      "Training loss at step 350: 0.2687\n",
      "Training accuracy: 0.9120 Validation accuracy: 0.8964 Time taken: 124.04s\n",
      "Epoch 104/200\n",
      "Training loss at step 0: 0.2549\n",
      "Training loss at step 25: 0.2126\n",
      "Training loss at step 50: 0.3577\n",
      "Training loss at step 75: 0.1583\n",
      "Training loss at step 100: 0.1831\n",
      "Training loss at step 125: 0.3388\n",
      "Training loss at step 150: 0.3044\n",
      "Training loss at step 175: 0.2115\n",
      "Training loss at step 200: 0.2496\n",
      "Training loss at step 225: 0.2072\n",
      "Training loss at step 250: 0.3288\n",
      "Training loss at step 275: 0.1456\n",
      "Training loss at step 300: 0.2295\n",
      "Training loss at step 325: 0.2546\n",
      "Training loss at step 350: 0.2097\n",
      "Training accuracy: 0.9134 Validation accuracy: 0.8948 Time taken: 123.76s\n",
      "Epoch 105/200\n",
      "Training loss at step 0: 0.1390\n",
      "Training loss at step 25: 0.3169\n",
      "Training loss at step 50: 0.2432\n",
      "Training loss at step 75: 0.2277\n",
      "Training loss at step 100: 0.1418\n",
      "Training loss at step 125: 0.2715\n",
      "Training loss at step 150: 0.1459\n",
      "Training loss at step 175: 0.2664\n",
      "Training loss at step 200: 0.2235\n",
      "Training loss at step 225: 0.1827\n",
      "Training loss at step 250: 0.2524\n",
      "Training loss at step 275: 0.2050\n",
      "Training loss at step 300: 0.2912\n",
      "Training loss at step 325: 0.1822\n",
      "Training loss at step 350: 0.1814\n",
      "Training accuracy: 0.9126 Validation accuracy: 0.9012 Time taken: 123.67s\n",
      "Epoch 106/200\n",
      "Training loss at step 0: 0.2916\n",
      "Training loss at step 25: 0.2914\n",
      "Training loss at step 50: 0.2994\n",
      "Training loss at step 75: 0.2035\n",
      "Training loss at step 100: 0.2875\n",
      "Training loss at step 125: 0.1752\n",
      "Training loss at step 150: 0.2337\n",
      "Training loss at step 175: 0.2608\n",
      "Training loss at step 200: 0.3469\n",
      "Training loss at step 225: 0.2634\n",
      "Training loss at step 250: 0.2724\n",
      "Training loss at step 275: 0.2864\n",
      "Training loss at step 300: 0.2786\n",
      "Training loss at step 325: 0.2299\n",
      "Training loss at step 350: 0.2848\n",
      "Training accuracy: 0.9127 Validation accuracy: 0.8970 Time taken: 123.78s\n",
      "Epoch 107/200\n",
      "Training loss at step 0: 0.1717\n",
      "Training loss at step 25: 0.2473\n",
      "Training loss at step 50: 0.2404\n",
      "Training loss at step 75: 0.2345\n",
      "Training loss at step 100: 0.2363\n",
      "Training loss at step 125: 0.1967\n",
      "Training loss at step 150: 0.2350\n",
      "Training loss at step 175: 0.2014\n",
      "Training loss at step 200: 0.3377\n",
      "Training loss at step 225: 0.3115\n",
      "Training loss at step 250: 0.2754\n",
      "Training loss at step 275: 0.2381\n",
      "Training loss at step 300: 0.2545\n",
      "Training loss at step 325: 0.2524\n",
      "Training loss at step 350: 0.2638\n",
      "Training accuracy: 0.9160 Validation accuracy: 0.9018 Time taken: 123.93s\n",
      "Epoch 108/200\n",
      "Training loss at step 0: 0.2709\n",
      "Training loss at step 25: 0.2878\n",
      "Training loss at step 50: 0.2502\n",
      "Training loss at step 75: 0.2278\n",
      "Training loss at step 100: 0.2676\n",
      "Training loss at step 125: 0.2144\n",
      "Training loss at step 150: 0.4157\n",
      "Training loss at step 175: 0.1698\n",
      "Training loss at step 200: 0.2536\n",
      "Training loss at step 225: 0.2479\n",
      "Training loss at step 250: 0.2803\n",
      "Training loss at step 275: 0.2140\n",
      "Training loss at step 300: 0.1835\n",
      "Training loss at step 325: 0.2639\n",
      "Training loss at step 350: 0.2578\n",
      "Training accuracy: 0.9141 Validation accuracy: 0.8974 Time taken: 123.03s\n",
      "Epoch 109/200\n",
      "Training loss at step 0: 0.2551\n",
      "Training loss at step 25: 0.1814\n",
      "Training loss at step 50: 0.2430\n",
      "Training loss at step 75: 0.1885\n",
      "Training loss at step 100: 0.1468\n",
      "Training loss at step 125: 0.1773\n",
      "Training loss at step 150: 0.2108\n",
      "Training loss at step 175: 0.2187\n",
      "Training loss at step 200: 0.2328\n",
      "Training loss at step 225: 0.2046\n",
      "Training loss at step 250: 0.1978\n",
      "Training loss at step 275: 0.3084\n",
      "Training loss at step 300: 0.2210\n",
      "Training loss at step 325: 0.3000\n",
      "Training loss at step 350: 0.2652\n",
      "Training accuracy: 0.9166 Validation accuracy: 0.9010 Time taken: 123.26s\n",
      "Epoch 110/200\n",
      "Training loss at step 0: 0.1549\n",
      "Training loss at step 25: 0.3848\n",
      "Training loss at step 50: 0.2349\n",
      "Training loss at step 75: 0.0956\n",
      "Training loss at step 100: 0.3135\n",
      "Training loss at step 125: 0.2736\n",
      "Training loss at step 150: 0.2856\n",
      "Training loss at step 175: 0.2483\n",
      "Training loss at step 200: 0.2441\n",
      "Training loss at step 225: 0.1613\n",
      "Training loss at step 250: 0.2185\n",
      "Training loss at step 275: 0.2393\n",
      "Training loss at step 300: 0.1673\n",
      "Training loss at step 325: 0.2202\n",
      "Training loss at step 350: 0.2688\n",
      "Training accuracy: 0.9181 Validation accuracy: 0.9020 Time taken: 123.24s\n",
      "Epoch 111/200\n",
      "Training loss at step 0: 0.2730\n",
      "Training loss at step 25: 0.1831\n",
      "Training loss at step 50: 0.2833\n",
      "Training loss at step 75: 0.1872\n",
      "Training loss at step 100: 0.1716\n",
      "Training loss at step 125: 0.1551\n",
      "Training loss at step 150: 0.1843\n",
      "Training loss at step 175: 0.2624\n",
      "Training loss at step 200: 0.1800\n",
      "Training loss at step 225: 0.2181\n",
      "Training loss at step 250: 0.2481\n",
      "Training loss at step 275: 0.1995\n",
      "Training loss at step 300: 0.1885\n",
      "Training loss at step 325: 0.1649\n",
      "Training loss at step 350: 0.2270\n",
      "Training accuracy: 0.9156 Validation accuracy: 0.8998 Time taken: 123.66s\n",
      "Epoch 112/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 0: 0.2863\n",
      "Training loss at step 25: 0.1514\n",
      "Training loss at step 50: 0.1742\n",
      "Training loss at step 75: 0.2049\n",
      "Training loss at step 100: 0.1689\n",
      "Training loss at step 125: 0.3858\n",
      "Training loss at step 150: 0.3273\n",
      "Training loss at step 175: 0.1455\n",
      "Training loss at step 200: 0.2278\n",
      "Training loss at step 225: 0.2871\n",
      "Training loss at step 250: 0.1083\n",
      "Training loss at step 275: 0.1948\n",
      "Training loss at step 300: 0.3098\n",
      "Training loss at step 325: 0.2138\n",
      "Training loss at step 350: 0.2607\n",
      "Training accuracy: 0.9172 Validation accuracy: 0.9026 Time taken: 123.82s\n",
      "Epoch 113/200\n",
      "Training loss at step 0: 0.1620\n",
      "Training loss at step 25: 0.1321\n",
      "Training loss at step 50: 0.1996\n",
      "Training loss at step 75: 0.1566\n",
      "Training loss at step 100: 0.2830\n",
      "Training loss at step 125: 0.2040\n",
      "Training loss at step 150: 0.2969\n",
      "Training loss at step 175: 0.1498\n",
      "Training loss at step 200: 0.1856\n",
      "Training loss at step 225: 0.2875\n",
      "Training loss at step 250: 0.2487\n",
      "Training loss at step 275: 0.3051\n",
      "Training loss at step 300: 0.2141\n",
      "Training loss at step 325: 0.2109\n",
      "Training loss at step 350: 0.3427\n",
      "Training accuracy: 0.9193 Validation accuracy: 0.9032 Time taken: 123.46s\n",
      "Epoch 114/200\n",
      "Training loss at step 0: 0.2138\n",
      "Training loss at step 25: 0.2104\n",
      "Training loss at step 50: 0.2715\n",
      "Training loss at step 75: 0.2655\n",
      "Training loss at step 100: 0.2316\n",
      "Training loss at step 125: 0.2692\n",
      "Training loss at step 150: 0.1879\n",
      "Training loss at step 175: 0.2620\n",
      "Training loss at step 200: 0.2171\n",
      "Training loss at step 225: 0.1402\n",
      "Training loss at step 250: 0.3028\n",
      "Training loss at step 275: 0.2207\n",
      "Training loss at step 300: 0.3721\n",
      "Training loss at step 325: 0.3085\n",
      "Training loss at step 350: 0.2088\n",
      "Training accuracy: 0.9190 Validation accuracy: 0.8996 Time taken: 123.74s\n",
      "Epoch 115/200\n",
      "Training loss at step 0: 0.1754\n",
      "Training loss at step 25: 0.2000\n",
      "Training loss at step 50: 0.2207\n",
      "Training loss at step 75: 0.2078\n",
      "Training loss at step 100: 0.1383\n",
      "Training loss at step 125: 0.1860\n",
      "Training loss at step 150: 0.1681\n",
      "Training loss at step 175: 0.1958\n",
      "Training loss at step 200: 0.2212\n",
      "Training loss at step 225: 0.2495\n",
      "Training loss at step 250: 0.2014\n",
      "Training loss at step 275: 0.2218\n",
      "Training loss at step 300: 0.2573\n",
      "Training loss at step 325: 0.2257\n",
      "Training loss at step 350: 0.1868\n",
      "Training accuracy: 0.9189 Validation accuracy: 0.9036 Time taken: 123.47s\n",
      "Epoch 116/200\n",
      "Training loss at step 0: 0.1901\n",
      "Training loss at step 25: 0.1304\n",
      "Training loss at step 50: 0.1804\n",
      "Training loss at step 75: 0.3202\n",
      "Training loss at step 100: 0.4094\n",
      "Training loss at step 125: 0.2363\n",
      "Training loss at step 150: 0.2392\n",
      "Training loss at step 175: 0.2512\n",
      "Training loss at step 200: 0.2682\n",
      "Training loss at step 225: 0.2686\n",
      "Training loss at step 250: 0.2255\n",
      "Training loss at step 275: 0.2962\n",
      "Training loss at step 300: 0.2385\n",
      "Training loss at step 325: 0.2257\n",
      "Training loss at step 350: 0.2250\n",
      "Training accuracy: 0.9214 Validation accuracy: 0.8996 Time taken: 124.10s\n",
      "Epoch 117/200\n",
      "Training loss at step 0: 0.2567\n",
      "Training loss at step 25: 0.2510\n",
      "Training loss at step 50: 0.3279\n",
      "Training loss at step 75: 0.2708\n",
      "Training loss at step 100: 0.1521\n",
      "Training loss at step 125: 0.2524\n",
      "Training loss at step 150: 0.2652\n",
      "Training loss at step 175: 0.1245\n",
      "Training loss at step 200: 0.3646\n",
      "Training loss at step 225: 0.2549\n",
      "Training loss at step 250: 0.1907\n",
      "Training loss at step 275: 0.1678\n",
      "Training loss at step 300: 0.3266\n",
      "Training loss at step 325: 0.3958\n",
      "Training loss at step 350: 0.2357\n",
      "Training accuracy: 0.9184 Validation accuracy: 0.8996 Time taken: 123.43s\n",
      "Epoch 118/200\n",
      "Training loss at step 0: 0.2448\n",
      "Training loss at step 25: 0.2430\n",
      "Training loss at step 50: 0.2659\n",
      "Training loss at step 75: 0.2605\n",
      "Training loss at step 100: 0.2657\n",
      "Training loss at step 125: 0.2104\n",
      "Training loss at step 150: 0.2599\n",
      "Training loss at step 175: 0.3128\n",
      "Training loss at step 200: 0.1665\n",
      "Training loss at step 225: 0.3089\n",
      "Training loss at step 250: 0.2123\n",
      "Training loss at step 275: 0.1694\n",
      "Training loss at step 300: 0.1621\n",
      "Training loss at step 325: 0.1812\n",
      "Training loss at step 350: 0.3467\n",
      "Training accuracy: 0.9206 Validation accuracy: 0.8990 Time taken: 123.69s\n",
      "Epoch 119/200\n",
      "Training loss at step 0: 0.1451\n",
      "Training loss at step 25: 0.2877\n",
      "Training loss at step 50: 0.2745\n",
      "Training loss at step 75: 0.1663\n",
      "Training loss at step 100: 0.3765\n",
      "Training loss at step 125: 0.2329\n",
      "Training loss at step 150: 0.1533\n",
      "Training loss at step 175: 0.2406\n",
      "Training loss at step 200: 0.3368\n",
      "Training loss at step 225: 0.1549\n",
      "Training loss at step 250: 0.2039\n",
      "Training loss at step 275: 0.1938\n",
      "Training loss at step 300: 0.2946\n",
      "Training loss at step 325: 0.1598\n",
      "Training loss at step 350: 0.3129\n",
      "Training accuracy: 0.9209 Validation accuracy: 0.9036 Time taken: 123.43s\n",
      "Epoch 120/200\n",
      "Training loss at step 0: 0.3665\n",
      "Training loss at step 25: 0.2168\n",
      "Training loss at step 50: 0.2978\n",
      "Training loss at step 75: 0.1772\n",
      "Training loss at step 100: 0.2948\n",
      "Training loss at step 125: 0.2938\n",
      "Training loss at step 150: 0.2365\n",
      "Training loss at step 175: 0.2781\n",
      "Training loss at step 200: 0.2061\n",
      "Training loss at step 225: 0.4064\n",
      "Training loss at step 250: 0.1213\n",
      "Training loss at step 275: 0.2691\n",
      "Training loss at step 300: 0.1775\n",
      "Training loss at step 325: 0.2207\n",
      "Training loss at step 350: 0.2217\n",
      "Training accuracy: 0.9203 Validation accuracy: 0.9008 Time taken: 123.51s\n",
      "Epoch 121/200\n",
      "Training loss at step 0: 0.2373\n",
      "Training loss at step 25: 0.1914\n",
      "Training loss at step 50: 0.2299\n",
      "Training loss at step 75: 0.3402\n",
      "Training loss at step 100: 0.2272\n",
      "Training loss at step 125: 0.1931\n",
      "Training loss at step 150: 0.1795\n",
      "Training loss at step 175: 0.1853\n",
      "Training loss at step 200: 0.2259\n",
      "Training loss at step 225: 0.1751\n",
      "Training loss at step 250: 0.2107\n",
      "Training loss at step 275: 0.1957\n",
      "Training loss at step 300: 0.1634\n",
      "Training loss at step 325: 0.3109\n",
      "Training loss at step 350: 0.2123\n",
      "Training accuracy: 0.9203 Validation accuracy: 0.9024 Time taken: 123.87s\n",
      "Epoch 122/200\n",
      "Training loss at step 0: 0.2563\n",
      "Training loss at step 25: 0.1535\n",
      "Training loss at step 50: 0.1493\n",
      "Training loss at step 75: 0.2411\n",
      "Training loss at step 100: 0.3084\n",
      "Training loss at step 125: 0.2350\n",
      "Training loss at step 150: 0.3123\n",
      "Training loss at step 175: 0.1391\n",
      "Training loss at step 200: 0.1794\n",
      "Training loss at step 225: 0.2026\n",
      "Training loss at step 250: 0.1968\n",
      "Training loss at step 275: 0.1803\n",
      "Training loss at step 300: 0.2246\n",
      "Training loss at step 325: 0.2956\n",
      "Training loss at step 350: 0.1862\n",
      "Training accuracy: 0.9217 Validation accuracy: 0.8974 Time taken: 123.21s\n",
      "Epoch 123/200\n",
      "Training loss at step 0: 0.1669\n",
      "Training loss at step 25: 0.1946\n",
      "Training loss at step 50: 0.1663\n",
      "Training loss at step 75: 0.2496\n",
      "Training loss at step 100: 0.2139\n",
      "Training loss at step 125: 0.2756\n",
      "Training loss at step 150: 0.2919\n",
      "Training loss at step 175: 0.2016\n",
      "Training loss at step 200: 0.1968\n",
      "Training loss at step 225: 0.2768\n",
      "Training loss at step 250: 0.1526\n",
      "Training loss at step 275: 0.2610\n",
      "Training loss at step 300: 0.2521\n",
      "Training loss at step 325: 0.1929\n",
      "Training loss at step 350: 0.2398\n",
      "Training accuracy: 0.9222 Validation accuracy: 0.9032 Time taken: 123.33s\n",
      "Epoch 124/200\n",
      "Training loss at step 0: 0.1733\n",
      "Training loss at step 25: 0.2474\n",
      "Training loss at step 50: 0.3249\n",
      "Training loss at step 75: 0.3374\n",
      "Training loss at step 100: 0.1748\n",
      "Training loss at step 125: 0.2435\n",
      "Training loss at step 150: 0.2008\n",
      "Training loss at step 175: 0.2250\n",
      "Training loss at step 200: 0.1289\n",
      "Training loss at step 225: 0.3019\n",
      "Training loss at step 250: 0.1535\n",
      "Training loss at step 275: 0.1948\n",
      "Training loss at step 300: 0.2595\n",
      "Training loss at step 325: 0.2107\n",
      "Training loss at step 350: 0.2214\n",
      "Training accuracy: 0.9214 Validation accuracy: 0.9016 Time taken: 123.60s\n",
      "Epoch 125/200\n",
      "Training loss at step 0: 0.2478\n",
      "Training loss at step 25: 0.2985\n",
      "Training loss at step 50: 0.1436\n",
      "Training loss at step 75: 0.2344\n",
      "Training loss at step 100: 0.2405\n",
      "Training loss at step 125: 0.1527\n",
      "Training loss at step 150: 0.2850\n",
      "Training loss at step 175: 0.1699\n",
      "Training loss at step 200: 0.3291\n",
      "Training loss at step 225: 0.2313\n",
      "Training loss at step 250: 0.2488\n",
      "Training loss at step 275: 0.1374\n",
      "Training loss at step 300: 0.2917\n",
      "Training loss at step 325: 0.2867\n",
      "Training loss at step 350: 0.1801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9206 Validation accuracy: 0.9034 Time taken: 123.75s\n",
      "Epoch 126/200\n",
      "Training loss at step 0: 0.1896\n",
      "Training loss at step 25: 0.2067\n",
      "Training loss at step 50: 0.2203\n",
      "Training loss at step 75: 0.2165\n",
      "Training loss at step 100: 0.1595\n",
      "Training loss at step 125: 0.2685\n",
      "Training loss at step 150: 0.2318\n",
      "Training loss at step 175: 0.3470\n",
      "Training loss at step 200: 0.2788\n",
      "Training loss at step 225: 0.2103\n",
      "Training loss at step 250: 0.1903\n",
      "Training loss at step 275: 0.1724\n",
      "Training loss at step 300: 0.2106\n",
      "Training loss at step 325: 0.1821\n",
      "Training loss at step 350: 0.2405\n",
      "Training accuracy: 0.9218 Validation accuracy: 0.9028 Time taken: 123.50s\n",
      "Epoch 127/200\n",
      "Training loss at step 0: 0.2087\n",
      "Training loss at step 25: 0.2159\n",
      "Training loss at step 50: 0.1227\n",
      "Training loss at step 75: 0.3170\n",
      "Training loss at step 100: 0.2676\n",
      "Training loss at step 125: 0.2272\n",
      "Training loss at step 150: 0.3702\n",
      "Training loss at step 175: 0.2834\n",
      "Training loss at step 200: 0.2327\n",
      "Training loss at step 225: 0.2099\n",
      "Training loss at step 250: 0.1320\n",
      "Training loss at step 275: 0.1696\n",
      "Training loss at step 300: 0.1558\n",
      "Training loss at step 325: 0.2364\n",
      "Training loss at step 350: 0.2477\n",
      "Training accuracy: 0.9216 Validation accuracy: 0.9006 Time taken: 123.49s\n",
      "Epoch 128/200\n",
      "Training loss at step 0: 0.2006\n",
      "Training loss at step 25: 0.2066\n",
      "Training loss at step 50: 0.2805\n",
      "Training loss at step 75: 0.1482\n",
      "Training loss at step 100: 0.2205\n",
      "Training loss at step 125: 0.2270\n",
      "Training loss at step 150: 0.2035\n",
      "Training loss at step 175: 0.2240\n",
      "Training loss at step 200: 0.1360\n",
      "Training loss at step 225: 0.2328\n",
      "Training loss at step 250: 0.1998\n",
      "Training loss at step 275: 0.3167\n",
      "Training loss at step 300: 0.2599\n",
      "Training loss at step 325: 0.2987\n",
      "Training loss at step 350: 0.3072\n",
      "Training accuracy: 0.9228 Validation accuracy: 0.9018 Time taken: 123.04s\n",
      "Epoch 129/200\n",
      "Training loss at step 0: 0.1582\n",
      "Training loss at step 25: 0.2018\n",
      "Training loss at step 50: 0.1947\n",
      "Training loss at step 75: 0.1426\n",
      "Training loss at step 100: 0.2770\n",
      "Training loss at step 125: 0.1977\n",
      "Training loss at step 150: 0.2201\n",
      "Training loss at step 175: 0.1936\n",
      "Training loss at step 200: 0.2870\n",
      "Training loss at step 225: 0.2640\n",
      "Training loss at step 250: 0.1177\n",
      "Training loss at step 275: 0.2781\n",
      "Training loss at step 300: 0.2279\n",
      "Training loss at step 325: 0.2317\n",
      "Training loss at step 350: 0.3013\n",
      "Training accuracy: 0.9226 Validation accuracy: 0.8972 Time taken: 123.25s\n",
      "Epoch 130/200\n",
      "Training loss at step 0: 0.2218\n",
      "Training loss at step 25: 0.2269\n",
      "Training loss at step 50: 0.2276\n",
      "Training loss at step 75: 0.1922\n",
      "Training loss at step 100: 0.2299\n",
      "Training loss at step 125: 0.1889\n",
      "Training loss at step 150: 0.1602\n",
      "Training loss at step 175: 0.1733\n",
      "Training loss at step 200: 0.1793\n",
      "Training loss at step 225: 0.1851\n",
      "Training loss at step 250: 0.2700\n",
      "Training loss at step 275: 0.1872\n",
      "Training loss at step 300: 0.2652\n",
      "Training loss at step 325: 0.1774\n",
      "Training loss at step 350: 0.1965\n",
      "Training accuracy: 0.9226 Validation accuracy: 0.9036 Time taken: 124.19s\n",
      "Epoch 131/200\n",
      "Training loss at step 0: 0.3575\n",
      "Training loss at step 25: 0.2375\n",
      "Training loss at step 50: 0.1756\n",
      "Training loss at step 75: 0.1985\n",
      "Training loss at step 100: 0.2057\n",
      "Training loss at step 125: 0.2572\n",
      "Training loss at step 150: 0.1814\n",
      "Training loss at step 175: 0.1849\n",
      "Training loss at step 200: 0.2295\n",
      "Training loss at step 225: 0.1125\n",
      "Training loss at step 250: 0.1122\n",
      "Training loss at step 275: 0.1632\n",
      "Training loss at step 300: 0.1392\n",
      "Training loss at step 325: 0.1882\n",
      "Training loss at step 350: 0.2355\n",
      "Training accuracy: 0.9230 Validation accuracy: 0.8996 Time taken: 123.66s\n",
      "Epoch 132/200\n",
      "Training loss at step 0: 0.2531\n",
      "Training loss at step 25: 0.2876\n",
      "Training loss at step 50: 0.1865\n",
      "Training loss at step 75: 0.1272\n",
      "Training loss at step 100: 0.2322\n",
      "Training loss at step 125: 0.2345\n",
      "Training loss at step 150: 0.2025\n",
      "Training loss at step 175: 0.2047\n",
      "Training loss at step 200: 0.3607\n",
      "Training loss at step 225: 0.1824\n",
      "Training loss at step 250: 0.1649\n",
      "Training loss at step 275: 0.2287\n",
      "Training loss at step 300: 0.1745\n",
      "Training loss at step 325: 0.1839\n",
      "Training loss at step 350: 0.2262\n",
      "Training accuracy: 0.9250 Validation accuracy: 0.9016 Time taken: 123.85s\n",
      "Epoch 133/200\n",
      "Training loss at step 0: 0.2606\n",
      "Training loss at step 25: 0.1798\n",
      "Training loss at step 50: 0.1963\n",
      "Training loss at step 75: 0.2268\n",
      "Training loss at step 100: 0.2104\n",
      "Training loss at step 125: 0.2471\n",
      "Training loss at step 150: 0.1956\n",
      "Training loss at step 175: 0.1832\n",
      "Training loss at step 200: 0.2278\n",
      "Training loss at step 225: 0.3062\n",
      "Training loss at step 250: 0.2437\n",
      "Training loss at step 275: 0.2975\n",
      "Training loss at step 300: 0.1578\n",
      "Training loss at step 325: 0.1777\n",
      "Training loss at step 350: 0.2400\n",
      "Training accuracy: 0.9234 Validation accuracy: 0.9050 Time taken: 123.40s\n",
      "Epoch 134/200\n",
      "Training loss at step 0: 0.2471\n",
      "Training loss at step 25: 0.2119\n",
      "Training loss at step 50: 0.2328\n",
      "Training loss at step 75: 0.2043\n",
      "Training loss at step 100: 0.2063\n",
      "Training loss at step 125: 0.1838\n",
      "Training loss at step 150: 0.1615\n",
      "Training loss at step 175: 0.1663\n",
      "Training loss at step 200: 0.2461\n",
      "Training loss at step 225: 0.1378\n",
      "Training loss at step 250: 0.1568\n",
      "Training loss at step 275: 0.2053\n",
      "Training loss at step 300: 0.2480\n",
      "Training loss at step 325: 0.3967\n",
      "Training loss at step 350: 0.1434\n",
      "Training accuracy: 0.9229 Validation accuracy: 0.8994 Time taken: 123.26s\n",
      "Epoch 135/200\n",
      "Training loss at step 0: 0.1709\n",
      "Training loss at step 25: 0.1976\n",
      "Training loss at step 50: 0.1641\n",
      "Training loss at step 75: 0.2527\n",
      "Training loss at step 100: 0.1180\n",
      "Training loss at step 125: 0.1370\n",
      "Training loss at step 150: 0.1831\n",
      "Training loss at step 175: 0.1757\n",
      "Training loss at step 200: 0.1493\n",
      "Training loss at step 225: 0.2725\n",
      "Training loss at step 250: 0.3251\n",
      "Training loss at step 275: 0.1705\n",
      "Training loss at step 300: 0.2539\n",
      "Training loss at step 325: 0.2226\n",
      "Training loss at step 350: 0.2241\n",
      "Training accuracy: 0.9253 Validation accuracy: 0.9020 Time taken: 123.64s\n",
      "Epoch 136/200\n",
      "Training loss at step 0: 0.1058\n",
      "Training loss at step 25: 0.1662\n",
      "Training loss at step 50: 0.1330\n",
      "Training loss at step 75: 0.1380\n",
      "Training loss at step 100: 0.3047\n",
      "Training loss at step 125: 0.1781\n",
      "Training loss at step 150: 0.2203\n",
      "Training loss at step 175: 0.1502\n",
      "Training loss at step 200: 0.1709\n",
      "Training loss at step 225: 0.2503\n",
      "Training loss at step 250: 0.1321\n",
      "Training loss at step 275: 0.1797\n",
      "Training loss at step 300: 0.1280\n",
      "Training loss at step 325: 0.2214\n",
      "Training loss at step 350: 0.2178\n",
      "Training accuracy: 0.9242 Validation accuracy: 0.9016 Time taken: 123.97s\n",
      "Epoch 137/200\n",
      "Training loss at step 0: 0.1550\n",
      "Training loss at step 25: 0.2110\n",
      "Training loss at step 50: 0.2008\n",
      "Training loss at step 75: 0.1804\n",
      "Training loss at step 100: 0.0848\n",
      "Training loss at step 125: 0.2609\n",
      "Training loss at step 150: 0.2740\n",
      "Training loss at step 175: 0.2053\n",
      "Training loss at step 200: 0.2014\n",
      "Training loss at step 225: 0.2393\n",
      "Training loss at step 250: 0.2369\n",
      "Training loss at step 275: 0.1786\n",
      "Training loss at step 300: 0.2633\n",
      "Training loss at step 325: 0.1628\n",
      "Training loss at step 350: 0.2303\n",
      "Training accuracy: 0.9233 Validation accuracy: 0.8982 Time taken: 123.45s\n",
      "Epoch 138/200\n",
      "Training loss at step 0: 0.2133\n",
      "Training loss at step 25: 0.1937\n",
      "Training loss at step 50: 0.2669\n",
      "Training loss at step 75: 0.2546\n",
      "Training loss at step 100: 0.2400\n",
      "Training loss at step 125: 0.1710\n",
      "Training loss at step 150: 0.2982\n",
      "Training loss at step 175: 0.2407\n",
      "Training loss at step 200: 0.1096\n",
      "Training loss at step 225: 0.2153\n",
      "Training loss at step 250: 0.3458\n",
      "Training loss at step 275: 0.3015\n",
      "Training loss at step 300: 0.2839\n",
      "Training loss at step 325: 0.3058\n",
      "Training loss at step 350: 0.2767\n",
      "Training accuracy: 0.9271 Validation accuracy: 0.9048 Time taken: 123.17s\n",
      "Epoch 139/200\n",
      "Training loss at step 0: 0.4251\n",
      "Training loss at step 25: 0.1547\n",
      "Training loss at step 50: 0.2714\n",
      "Training loss at step 75: 0.1635\n",
      "Training loss at step 100: 0.3043\n",
      "Training loss at step 125: 0.1737\n",
      "Training loss at step 150: 0.1980\n",
      "Training loss at step 175: 0.1920\n",
      "Training loss at step 200: 0.1906\n",
      "Training loss at step 225: 0.2017\n",
      "Training loss at step 250: 0.1408\n",
      "Training loss at step 275: 0.3212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 300: 0.2513\n",
      "Training loss at step 325: 0.2608\n",
      "Training loss at step 350: 0.2298\n",
      "Training accuracy: 0.9249 Validation accuracy: 0.9016 Time taken: 122.86s\n",
      "Epoch 140/200\n",
      "Training loss at step 0: 0.1192\n",
      "Training loss at step 25: 0.1678\n",
      "Training loss at step 50: 0.1607\n",
      "Training loss at step 75: 0.1722\n",
      "Training loss at step 100: 0.2209\n",
      "Training loss at step 125: 0.2652\n",
      "Training loss at step 150: 0.2152\n",
      "Training loss at step 175: 0.2265\n",
      "Training loss at step 200: 0.1201\n",
      "Training loss at step 225: 0.1142\n",
      "Training loss at step 250: 0.2159\n",
      "Training loss at step 275: 0.2706\n",
      "Training loss at step 300: 0.2957\n",
      "Training loss at step 325: 0.3149\n",
      "Training loss at step 350: 0.2540\n",
      "Training accuracy: 0.9253 Validation accuracy: 0.9014 Time taken: 124.00s\n",
      "Epoch 141/200\n",
      "Training loss at step 0: 0.2797\n",
      "Training loss at step 25: 0.2417\n",
      "Training loss at step 50: 0.2591\n",
      "Training loss at step 75: 0.2995\n",
      "Training loss at step 100: 0.1883\n",
      "Training loss at step 125: 0.1955\n",
      "Training loss at step 150: 0.2386\n",
      "Training loss at step 175: 0.1858\n",
      "Training loss at step 200: 0.1819\n",
      "Training loss at step 225: 0.2289\n",
      "Training loss at step 250: 0.2303\n",
      "Training loss at step 275: 0.2274\n",
      "Training loss at step 300: 0.2408\n",
      "Training loss at step 325: 0.1425\n",
      "Training loss at step 350: 0.1895\n",
      "Training accuracy: 0.9253 Validation accuracy: 0.8976 Time taken: 123.67s\n",
      "Epoch 142/200\n",
      "Training loss at step 0: 0.1828\n",
      "Training loss at step 25: 0.1742\n",
      "Training loss at step 50: 0.2141\n",
      "Training loss at step 75: 0.1546\n",
      "Training loss at step 100: 0.1783\n",
      "Training loss at step 125: 0.2790\n",
      "Training loss at step 150: 0.2281\n",
      "Training loss at step 175: 0.2929\n",
      "Training loss at step 200: 0.1841\n",
      "Training loss at step 225: 0.2546\n",
      "Training loss at step 250: 0.1910\n",
      "Training loss at step 275: 0.1885\n",
      "Training loss at step 300: 0.2240\n",
      "Training loss at step 325: 0.1811\n",
      "Training loss at step 350: 0.1625\n",
      "Training accuracy: 0.9246 Validation accuracy: 0.9010 Time taken: 123.10s\n",
      "Epoch 143/200\n",
      "Training loss at step 0: 0.2266\n",
      "Training loss at step 25: 0.1511\n",
      "Training loss at step 50: 0.2817\n",
      "Training loss at step 75: 0.1562\n",
      "Training loss at step 100: 0.2034\n",
      "Training loss at step 125: 0.1685\n",
      "Training loss at step 150: 0.2662\n",
      "Training loss at step 175: 0.2269\n",
      "Training loss at step 200: 0.3194\n",
      "Training loss at step 225: 0.0964\n",
      "Training loss at step 250: 0.1494\n",
      "Training loss at step 275: 0.2489\n",
      "Training loss at step 300: 0.1411\n",
      "Training loss at step 325: 0.1817\n",
      "Training loss at step 350: 0.1313\n",
      "Training accuracy: 0.9270 Validation accuracy: 0.9004 Time taken: 122.87s\n",
      "Epoch 144/200\n",
      "Training loss at step 0: 0.2953\n",
      "Training loss at step 25: 0.2194\n",
      "Training loss at step 50: 0.2364\n",
      "Training loss at step 75: 0.1661\n",
      "Training loss at step 100: 0.1423\n",
      "Training loss at step 125: 0.2235\n",
      "Training loss at step 150: 0.2438\n",
      "Training loss at step 175: 0.1788\n",
      "Training loss at step 200: 0.2504\n",
      "Training loss at step 225: 0.3826\n",
      "Training loss at step 250: 0.1421\n",
      "Training loss at step 275: 0.2594\n",
      "Training loss at step 300: 0.1550\n",
      "Training loss at step 325: 0.1452\n",
      "Training loss at step 350: 0.1583\n",
      "Training accuracy: 0.9274 Validation accuracy: 0.9018 Time taken: 122.92s\n",
      "Epoch 145/200\n",
      "Training loss at step 0: 0.2568\n",
      "Training loss at step 25: 0.1738\n",
      "Training loss at step 50: 0.1353\n",
      "Training loss at step 75: 0.1945\n",
      "Training loss at step 100: 0.2329\n",
      "Training loss at step 125: 0.2268\n",
      "Training loss at step 150: 0.2131\n",
      "Training loss at step 175: 0.1981\n",
      "Training loss at step 200: 0.1740\n",
      "Training loss at step 225: 0.2721\n",
      "Training loss at step 250: 0.2220\n",
      "Training loss at step 275: 0.1603\n",
      "Training loss at step 300: 0.2927\n",
      "Training loss at step 325: 0.2996\n",
      "Training loss at step 350: 0.1801\n",
      "Training accuracy: 0.9263 Validation accuracy: 0.9044 Time taken: 123.08s\n",
      "Epoch 146/200\n",
      "Training loss at step 0: 0.2189\n",
      "Training loss at step 25: 0.1834\n",
      "Training loss at step 50: 0.1891\n",
      "Training loss at step 75: 0.1711\n",
      "Training loss at step 100: 0.1659\n",
      "Training loss at step 125: 0.2202\n",
      "Training loss at step 150: 0.2009\n",
      "Training loss at step 175: 0.1675\n",
      "Training loss at step 200: 0.2509\n",
      "Training loss at step 225: 0.1546\n",
      "Training loss at step 250: 0.2830\n",
      "Training loss at step 275: 0.1865\n",
      "Training loss at step 300: 0.1670\n",
      "Training loss at step 325: 0.1881\n",
      "Training loss at step 350: 0.2154\n",
      "Training accuracy: 0.9262 Validation accuracy: 0.9024 Time taken: 124.34s\n",
      "Epoch 147/200\n",
      "Training loss at step 0: 0.2254\n",
      "Training loss at step 25: 0.2551\n",
      "Training loss at step 50: 0.2666\n",
      "Training loss at step 75: 0.1914\n",
      "Training loss at step 100: 0.1913\n",
      "Training loss at step 125: 0.1870\n",
      "Training loss at step 150: 0.3089\n",
      "Training loss at step 175: 0.3212\n",
      "Training loss at step 200: 0.2490\n",
      "Training loss at step 225: 0.1738\n",
      "Training loss at step 250: 0.1765\n",
      "Training loss at step 275: 0.2205\n",
      "Training loss at step 300: 0.2099\n",
      "Training loss at step 325: 0.1972\n",
      "Training loss at step 350: 0.1980\n",
      "Training accuracy: 0.9269 Validation accuracy: 0.9040 Time taken: 123.56s\n",
      "Epoch 148/200\n",
      "Training loss at step 0: 0.2899\n",
      "Training loss at step 25: 0.2479\n",
      "Training loss at step 50: 0.1693\n",
      "Training loss at step 75: 0.1679\n",
      "Training loss at step 100: 0.0955\n",
      "Training loss at step 125: 0.1206\n",
      "Training loss at step 150: 0.1635\n",
      "Training loss at step 175: 0.2017\n",
      "Training loss at step 200: 0.1958\n",
      "Training loss at step 225: 0.2821\n",
      "Training loss at step 250: 0.1255\n",
      "Training loss at step 275: 0.1558\n",
      "Training loss at step 300: 0.2001\n",
      "Training loss at step 325: 0.2865\n",
      "Training loss at step 350: 0.2253\n",
      "Training accuracy: 0.9280 Validation accuracy: 0.9002 Time taken: 124.11s\n",
      "Epoch 149/200\n",
      "Training loss at step 0: 0.2887\n",
      "Training loss at step 25: 0.2000\n",
      "Training loss at step 50: 0.1176\n",
      "Training loss at step 75: 0.2061\n",
      "Training loss at step 100: 0.1704\n",
      "Training loss at step 125: 0.1762\n",
      "Training loss at step 150: 0.1886\n",
      "Training loss at step 175: 0.2327\n",
      "Training loss at step 200: 0.2018\n",
      "Training loss at step 225: 0.1879\n",
      "Training loss at step 250: 0.1822\n",
      "Training loss at step 275: 0.1540\n",
      "Training loss at step 300: 0.0769\n",
      "Training loss at step 325: 0.2022\n",
      "Training loss at step 350: 0.1973\n",
      "Training accuracy: 0.9277 Validation accuracy: 0.8998 Time taken: 123.73s\n",
      "Epoch 150/200\n",
      "Training loss at step 0: 0.1549\n",
      "Training loss at step 25: 0.2372\n",
      "Training loss at step 50: 0.2493\n",
      "Training loss at step 75: 0.2010\n",
      "Training loss at step 100: 0.2086\n",
      "Training loss at step 125: 0.1958\n",
      "Training loss at step 150: 0.2470\n",
      "Training loss at step 175: 0.1281\n",
      "Training loss at step 200: 0.2126\n",
      "Training loss at step 225: 0.1835\n",
      "Training loss at step 250: 0.2950\n",
      "Training loss at step 275: 0.2450\n",
      "Training loss at step 300: 0.1896\n",
      "Training loss at step 325: 0.1764\n",
      "Training loss at step 350: 0.2941\n",
      "Training accuracy: 0.9281 Validation accuracy: 0.9022 Time taken: 122.93s\n",
      "Epoch 151/200\n",
      "Training loss at step 0: 0.2011\n",
      "Training loss at step 25: 0.1347\n",
      "Training loss at step 50: 0.1702\n",
      "Training loss at step 75: 0.2128\n",
      "Training loss at step 100: 0.2333\n",
      "Training loss at step 125: 0.2152\n",
      "Training loss at step 150: 0.2231\n",
      "Training loss at step 175: 0.1550\n",
      "Training loss at step 200: 0.2164\n",
      "Training loss at step 225: 0.2234\n",
      "Training loss at step 250: 0.1818\n",
      "Training loss at step 275: 0.2447\n",
      "Training loss at step 300: 0.4328\n",
      "Training loss at step 325: 0.2179\n",
      "Training loss at step 350: 0.2880\n",
      "Training accuracy: 0.9286 Validation accuracy: 0.9030 Time taken: 123.49s\n",
      "Epoch 152/200\n",
      "Training loss at step 0: 0.1356\n",
      "Training loss at step 25: 0.1965\n",
      "Training loss at step 50: 0.1358\n",
      "Training loss at step 75: 0.1949\n",
      "Training loss at step 100: 0.1094\n",
      "Training loss at step 125: 0.1876\n",
      "Training loss at step 150: 0.1894\n",
      "Training loss at step 175: 0.1965\n",
      "Training loss at step 200: 0.2426\n",
      "Training loss at step 225: 0.1986\n",
      "Training loss at step 250: 0.1481\n",
      "Training loss at step 275: 0.1733\n",
      "Training loss at step 300: 0.1973\n",
      "Training loss at step 325: 0.1662\n",
      "Training loss at step 350: 0.2557\n",
      "Training accuracy: 0.9279 Validation accuracy: 0.9022 Time taken: 122.76s\n",
      "Epoch 153/200\n",
      "Training loss at step 0: 0.2047\n",
      "Training loss at step 25: 0.2872\n",
      "Training loss at step 50: 0.1473\n",
      "Training loss at step 75: 0.1839\n",
      "Training loss at step 100: 0.2299\n",
      "Training loss at step 125: 0.1530\n",
      "Training loss at step 150: 0.1649\n",
      "Training loss at step 175: 0.2159\n",
      "Training loss at step 200: 0.1819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 225: 0.1473\n",
      "Training loss at step 250: 0.2534\n",
      "Training loss at step 275: 0.1313\n",
      "Training loss at step 300: 0.1977\n",
      "Training loss at step 325: 0.1227\n",
      "Training loss at step 350: 0.1883\n",
      "Training accuracy: 0.9304 Validation accuracy: 0.9028 Time taken: 123.29s\n",
      "Epoch 154/200\n",
      "Training loss at step 0: 0.1965\n",
      "Training loss at step 25: 0.1986\n",
      "Training loss at step 50: 0.2311\n",
      "Training loss at step 75: 0.1384\n",
      "Training loss at step 100: 0.1845\n",
      "Training loss at step 125: 0.1087\n",
      "Training loss at step 150: 0.2072\n",
      "Training loss at step 175: 0.2310\n",
      "Training loss at step 200: 0.2238\n",
      "Training loss at step 225: 0.1909\n",
      "Training loss at step 250: 0.2797\n",
      "Training loss at step 275: 0.1580\n",
      "Training loss at step 300: 0.2019\n",
      "Training loss at step 325: 0.1729\n",
      "Training loss at step 350: 0.2136\n",
      "Training accuracy: 0.9280 Validation accuracy: 0.9032 Time taken: 122.81s\n",
      "Epoch 155/200\n",
      "Training loss at step 0: 0.2146\n",
      "Training loss at step 25: 0.1799\n",
      "Training loss at step 50: 0.2178\n",
      "Training loss at step 75: 0.1938\n",
      "Training loss at step 100: 0.1641\n",
      "Training loss at step 125: 0.1341\n",
      "Training loss at step 150: 0.2393\n",
      "Training loss at step 175: 0.1429\n",
      "Training loss at step 200: 0.2161\n",
      "Training loss at step 225: 0.1277\n",
      "Training loss at step 250: 0.3359\n",
      "Training loss at step 275: 0.2321\n",
      "Training loss at step 300: 0.2106\n",
      "Training loss at step 325: 0.2666\n",
      "Training loss at step 350: 0.2104\n",
      "Training accuracy: 0.9303 Validation accuracy: 0.9038 Time taken: 122.71s\n",
      "Epoch 156/200\n",
      "Training loss at step 0: 0.1766\n",
      "Training loss at step 25: 0.1473\n",
      "Training loss at step 50: 0.3357\n",
      "Training loss at step 75: 0.1175\n",
      "Training loss at step 100: 0.1324\n",
      "Training loss at step 125: 0.2740\n",
      "Training loss at step 150: 0.1528\n",
      "Training loss at step 175: 0.2015\n",
      "Training loss at step 200: 0.2119\n",
      "Training loss at step 225: 0.2302\n",
      "Training loss at step 250: 0.1480\n",
      "Training loss at step 275: 0.1608\n",
      "Training loss at step 300: 0.2059\n",
      "Training loss at step 325: 0.1222\n",
      "Training loss at step 350: 0.1086\n",
      "Training accuracy: 0.9297 Validation accuracy: 0.9032 Time taken: 123.01s\n",
      "Epoch 157/200\n",
      "Training loss at step 0: 0.1804\n",
      "Training loss at step 25: 0.1603\n",
      "Training loss at step 50: 0.1190\n",
      "Training loss at step 75: 0.1998\n",
      "Training loss at step 100: 0.1621\n",
      "Training loss at step 125: 0.1789\n",
      "Training loss at step 150: 0.3446\n",
      "Training loss at step 175: 0.1382\n",
      "Training loss at step 200: 0.2137\n",
      "Training loss at step 225: 0.1079\n",
      "Training loss at step 250: 0.1975\n",
      "Training loss at step 275: 0.1232\n",
      "Training loss at step 300: 0.2527\n",
      "Training loss at step 325: 0.2488\n",
      "Training loss at step 350: 0.1787\n",
      "Training accuracy: 0.9303 Validation accuracy: 0.9022 Time taken: 123.40s\n",
      "Epoch 158/200\n",
      "Training loss at step 0: 0.2503\n",
      "Training loss at step 25: 0.1592\n",
      "Training loss at step 50: 0.1323\n",
      "Training loss at step 75: 0.1804\n",
      "Training loss at step 100: 0.1654\n",
      "Training loss at step 125: 0.1834\n",
      "Training loss at step 150: 0.1734\n",
      "Training loss at step 175: 0.1810\n",
      "Training loss at step 200: 0.1306\n",
      "Training loss at step 225: 0.2733\n",
      "Training loss at step 250: 0.1222\n",
      "Training loss at step 275: 0.1277\n",
      "Training loss at step 300: 0.2021\n",
      "Training loss at step 325: 0.2355\n",
      "Training loss at step 350: 0.2427\n",
      "Training accuracy: 0.9288 Validation accuracy: 0.9024 Time taken: 122.46s\n",
      "Epoch 159/200\n",
      "Training loss at step 0: 0.1485\n",
      "Training loss at step 25: 0.1128\n",
      "Training loss at step 50: 0.1674\n",
      "Training loss at step 75: 0.2044\n",
      "Training loss at step 100: 0.2035\n",
      "Training loss at step 125: 0.2047\n",
      "Training loss at step 150: 0.2334\n",
      "Training loss at step 175: 0.1611\n",
      "Training loss at step 200: 0.2418\n",
      "Training loss at step 225: 0.3281\n",
      "Training loss at step 250: 0.3247\n",
      "Training loss at step 275: 0.2549\n",
      "Training loss at step 300: 0.1936\n",
      "Training loss at step 325: 0.1446\n",
      "Training loss at step 350: 0.2486\n",
      "Training accuracy: 0.9306 Validation accuracy: 0.9030 Time taken: 123.21s\n",
      "Epoch 160/200\n",
      "Training loss at step 0: 0.2321\n",
      "Training loss at step 25: 0.1616\n",
      "Training loss at step 50: 0.1640\n",
      "Training loss at step 75: 0.1180\n",
      "Training loss at step 100: 0.1488\n",
      "Training loss at step 125: 0.2073\n",
      "Training loss at step 150: 0.1763\n",
      "Training loss at step 175: 0.2390\n",
      "Training loss at step 200: 0.1697\n",
      "Training loss at step 225: 0.2077\n",
      "Training loss at step 250: 0.2227\n",
      "Training loss at step 275: 0.1399\n",
      "Training loss at step 300: 0.1687\n",
      "Training loss at step 325: 0.2108\n",
      "Training loss at step 350: 0.1108\n",
      "Training accuracy: 0.9306 Validation accuracy: 0.9028 Time taken: 123.21s\n",
      "Epoch 161/200\n",
      "Training loss at step 0: 0.1207\n",
      "Training loss at step 25: 0.2565\n",
      "Training loss at step 50: 0.1796\n",
      "Training loss at step 75: 0.1248\n",
      "Training loss at step 100: 0.1498\n",
      "Training loss at step 125: 0.2026\n",
      "Training loss at step 150: 0.1481\n",
      "Training loss at step 175: 0.1324\n",
      "Training loss at step 200: 0.2285\n",
      "Training loss at step 225: 0.2046\n",
      "Training loss at step 250: 0.2168\n",
      "Training loss at step 275: 0.1699\n",
      "Training loss at step 300: 0.0990\n",
      "Training loss at step 325: 0.1359\n",
      "Training loss at step 350: 0.2177\n",
      "Training accuracy: 0.9307 Validation accuracy: 0.9032 Time taken: 123.43s\n",
      "Epoch 162/200\n",
      "Training loss at step 0: 0.2178\n",
      "Training loss at step 25: 0.1254\n",
      "Training loss at step 50: 0.2443\n",
      "Training loss at step 75: 0.2142\n",
      "Training loss at step 100: 0.1312\n",
      "Training loss at step 125: 0.1755\n",
      "Training loss at step 150: 0.2513\n",
      "Training loss at step 175: 0.2702\n",
      "Training loss at step 200: 0.2164\n",
      "Training loss at step 225: 0.1530\n",
      "Training loss at step 250: 0.2376\n",
      "Training loss at step 275: 0.2097\n",
      "Training loss at step 300: 0.2095\n",
      "Training loss at step 325: 0.2282\n",
      "Training loss at step 350: 0.1514\n",
      "Training accuracy: 0.9323 Validation accuracy: 0.9032 Time taken: 123.36s\n",
      "Epoch 163/200\n",
      "Training loss at step 0: 0.2451\n",
      "Training loss at step 25: 0.2096\n",
      "Training loss at step 50: 0.1909\n",
      "Training loss at step 75: 0.1316\n",
      "Training loss at step 100: 0.3320\n",
      "Training loss at step 125: 0.1675\n",
      "Training loss at step 150: 0.1673\n",
      "Training loss at step 175: 0.3738\n",
      "Training loss at step 200: 0.1958\n",
      "Training loss at step 225: 0.1856\n",
      "Training loss at step 250: 0.2004\n",
      "Training loss at step 275: 0.2341\n",
      "Training loss at step 300: 0.2049\n",
      "Training loss at step 325: 0.3906\n",
      "Training loss at step 350: 0.1537\n",
      "Training accuracy: 0.9309 Validation accuracy: 0.9030 Time taken: 122.94s\n",
      "Epoch 164/200\n",
      "Training loss at step 0: 0.1970\n",
      "Training loss at step 25: 0.1737\n",
      "Training loss at step 50: 0.2956\n",
      "Training loss at step 75: 0.2493\n",
      "Training loss at step 100: 0.1535\n",
      "Training loss at step 125: 0.1845\n",
      "Training loss at step 150: 0.2816\n",
      "Training loss at step 175: 0.2167\n",
      "Training loss at step 200: 0.1870\n",
      "Training loss at step 225: 0.1888\n",
      "Training loss at step 250: 0.1629\n",
      "Training loss at step 275: 0.1683\n",
      "Training loss at step 300: 0.1701\n",
      "Training loss at step 325: 0.2103\n",
      "Training loss at step 350: 0.2114\n",
      "Training accuracy: 0.9297 Validation accuracy: 0.9028 Time taken: 123.23s\n",
      "Epoch 165/200\n",
      "Training loss at step 0: 0.2303\n",
      "Training loss at step 25: 0.1747\n",
      "Training loss at step 50: 0.2004\n",
      "Training loss at step 75: 0.1591\n",
      "Training loss at step 100: 0.2155\n",
      "Training loss at step 125: 0.2474\n",
      "Training loss at step 150: 0.1855\n",
      "Training loss at step 175: 0.2575\n",
      "Training loss at step 200: 0.1215\n",
      "Training loss at step 225: 0.1808\n",
      "Training loss at step 250: 0.2152\n",
      "Training loss at step 275: 0.2198\n",
      "Training loss at step 300: 0.1663\n",
      "Training loss at step 325: 0.2828\n",
      "Training loss at step 350: 0.1272\n",
      "Training accuracy: 0.9316 Validation accuracy: 0.9040 Time taken: 123.60s\n",
      "Epoch 166/200\n",
      "Training loss at step 0: 0.2142\n",
      "Training loss at step 25: 0.1699\n",
      "Training loss at step 50: 0.2167\n",
      "Training loss at step 75: 0.2005\n",
      "Training loss at step 100: 0.1611\n",
      "Training loss at step 125: 0.1725\n",
      "Training loss at step 150: 0.1783\n",
      "Training loss at step 175: 0.2237\n",
      "Training loss at step 200: 0.2257\n",
      "Training loss at step 225: 0.1496\n",
      "Training loss at step 250: 0.2064\n",
      "Training loss at step 275: 0.1364\n",
      "Training loss at step 300: 0.2106\n",
      "Training loss at step 325: 0.1673\n",
      "Training loss at step 350: 0.2135\n",
      "Training accuracy: 0.9290 Validation accuracy: 0.9028 Time taken: 123.39s\n",
      "Epoch 167/200\n",
      "Training loss at step 0: 0.2556\n",
      "Training loss at step 25: 0.1815\n",
      "Training loss at step 50: 0.1098\n",
      "Training loss at step 75: 0.1930\n",
      "Training loss at step 100: 0.1824\n",
      "Training loss at step 125: 0.1688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 150: 0.1425\n",
      "Training loss at step 175: 0.2545\n",
      "Training loss at step 200: 0.1492\n",
      "Training loss at step 225: 0.1988\n",
      "Training loss at step 250: 0.1723\n",
      "Training loss at step 275: 0.2019\n",
      "Training loss at step 300: 0.1673\n",
      "Training loss at step 325: 0.1418\n",
      "Training loss at step 350: 0.2466\n",
      "Training accuracy: 0.9312 Validation accuracy: 0.9036 Time taken: 123.25s\n",
      "Epoch 168/200\n",
      "Training loss at step 0: 0.1467\n",
      "Training loss at step 25: 0.2295\n",
      "Training loss at step 50: 0.1147\n",
      "Training loss at step 75: 0.1175\n",
      "Training loss at step 100: 0.2270\n",
      "Training loss at step 125: 0.1817\n",
      "Training loss at step 150: 0.1739\n",
      "Training loss at step 175: 0.1349\n",
      "Training loss at step 200: 0.2620\n",
      "Training loss at step 225: 0.1626\n",
      "Training loss at step 250: 0.1833\n",
      "Training loss at step 275: 0.1305\n",
      "Training loss at step 300: 0.1557\n",
      "Training loss at step 325: 0.1916\n",
      "Training loss at step 350: 0.1465\n",
      "Training accuracy: 0.9318 Validation accuracy: 0.9038 Time taken: 123.11s\n",
      "Epoch 169/200\n",
      "Training loss at step 0: 0.1874\n",
      "Training loss at step 25: 0.1575\n",
      "Training loss at step 50: 0.1942\n",
      "Training loss at step 75: 0.2349\n",
      "Training loss at step 100: 0.1691\n",
      "Training loss at step 125: 0.1786\n",
      "Training loss at step 150: 0.1425\n",
      "Training loss at step 175: 0.2175\n",
      "Training loss at step 200: 0.2153\n",
      "Training loss at step 225: 0.1704\n",
      "Training loss at step 250: 0.1873\n",
      "Training loss at step 275: 0.1800\n",
      "Training loss at step 300: 0.0968\n",
      "Training loss at step 325: 0.1165\n",
      "Training loss at step 350: 0.1774\n",
      "Training accuracy: 0.9300 Validation accuracy: 0.9040 Time taken: 123.64s\n",
      "Epoch 170/200\n",
      "Training loss at step 0: 0.2911\n",
      "Training loss at step 25: 0.1924\n",
      "Training loss at step 50: 0.2358\n",
      "Training loss at step 75: 0.1896\n",
      "Training loss at step 100: 0.1177\n",
      "Training loss at step 125: 0.2024\n",
      "Training loss at step 150: 0.2277\n",
      "Training loss at step 175: 0.1583\n",
      "Training loss at step 200: 0.1847\n",
      "Training loss at step 225: 0.1417\n",
      "Training loss at step 250: 0.1340\n",
      "Training loss at step 275: 0.2736\n",
      "Training loss at step 300: 0.1662\n",
      "Training loss at step 325: 0.1576\n",
      "Training loss at step 350: 0.2154\n",
      "Training accuracy: 0.9306 Validation accuracy: 0.9028 Time taken: 123.26s\n",
      "Epoch 171/200\n",
      "Training loss at step 0: 0.1682\n",
      "Training loss at step 25: 0.2520\n",
      "Training loss at step 50: 0.2666\n",
      "Training loss at step 75: 0.1570\n",
      "Training loss at step 100: 0.1492\n",
      "Training loss at step 125: 0.1873\n",
      "Training loss at step 150: 0.1416\n",
      "Training loss at step 175: 0.1365\n",
      "Training loss at step 200: 0.1920\n",
      "Training loss at step 225: 0.2182\n",
      "Training loss at step 250: 0.2955\n",
      "Training loss at step 275: 0.2000\n",
      "Training loss at step 300: 0.2355\n",
      "Training loss at step 325: 0.1854\n",
      "Training loss at step 350: 0.1800\n",
      "Training accuracy: 0.9288 Validation accuracy: 0.9036 Time taken: 123.69s\n",
      "Epoch 172/200\n",
      "Training loss at step 0: 0.2709\n",
      "Training loss at step 25: 0.3254\n",
      "Training loss at step 50: 0.2523\n",
      "Training loss at step 75: 0.1986\n",
      "Training loss at step 100: 0.2242\n",
      "Training loss at step 125: 0.1417\n",
      "Training loss at step 150: 0.1057\n",
      "Training loss at step 175: 0.2184\n",
      "Training loss at step 200: 0.1474\n",
      "Training loss at step 225: 0.1026\n",
      "Training loss at step 250: 0.1813\n",
      "Training loss at step 275: 0.2249\n",
      "Training loss at step 300: 0.0598\n",
      "Training loss at step 325: 0.1523\n",
      "Training loss at step 350: 0.3204\n",
      "Training accuracy: 0.9309 Validation accuracy: 0.9028 Time taken: 123.63s\n",
      "Epoch 173/200\n",
      "Training loss at step 0: 0.3040\n",
      "Training loss at step 25: 0.2460\n",
      "Training loss at step 50: 0.1244\n",
      "Training loss at step 75: 0.1658\n",
      "Training loss at step 100: 0.1941\n",
      "Training loss at step 125: 0.1252\n",
      "Training loss at step 150: 0.1511\n",
      "Training loss at step 175: 0.2472\n",
      "Training loss at step 200: 0.1679\n",
      "Training loss at step 225: 0.2102\n",
      "Training loss at step 250: 0.2337\n",
      "Training loss at step 275: 0.2417\n",
      "Training loss at step 300: 0.2439\n",
      "Training loss at step 325: 0.2287\n",
      "Training loss at step 350: 0.1770\n",
      "Training accuracy: 0.9310 Validation accuracy: 0.9038 Time taken: 123.87s\n",
      "Epoch 174/200\n",
      "Training loss at step 0: 0.2172\n",
      "Training loss at step 25: 0.0962\n",
      "Training loss at step 50: 0.1714\n",
      "Training loss at step 75: 0.3130\n",
      "Training loss at step 100: 0.2010\n",
      "Training loss at step 125: 0.1437\n",
      "Training loss at step 150: 0.1593\n",
      "Training loss at step 175: 0.2094\n",
      "Training loss at step 200: 0.2147\n",
      "Training loss at step 225: 0.2631\n",
      "Training loss at step 250: 0.1504\n",
      "Training loss at step 275: 0.2397\n",
      "Training loss at step 300: 0.2695\n",
      "Training loss at step 325: 0.2089\n",
      "Training loss at step 350: 0.2323\n",
      "Training accuracy: 0.9315 Validation accuracy: 0.9048 Time taken: 123.84s\n",
      "Epoch 175/200\n",
      "Training loss at step 0: 0.2174\n",
      "Training loss at step 25: 0.2082\n",
      "Training loss at step 50: 0.2766\n",
      "Training loss at step 75: 0.2841\n",
      "Training loss at step 100: 0.2350\n",
      "Training loss at step 125: 0.1491\n",
      "Training loss at step 150: 0.1263\n",
      "Training loss at step 175: 0.2744\n",
      "Training loss at step 200: 0.2618\n",
      "Training loss at step 225: 0.2623\n",
      "Training loss at step 250: 0.1057\n",
      "Training loss at step 275: 0.2219\n",
      "Training loss at step 300: 0.2351\n",
      "Training loss at step 325: 0.2435\n",
      "Training loss at step 350: 0.2021\n",
      "Training accuracy: 0.9299 Validation accuracy: 0.9044 Time taken: 123.48s\n",
      "Epoch 176/200\n",
      "Training loss at step 0: 0.0993\n",
      "Training loss at step 25: 0.1563\n",
      "Training loss at step 50: 0.1477\n",
      "Training loss at step 75: 0.1682\n",
      "Training loss at step 100: 0.2437\n",
      "Training loss at step 125: 0.2347\n",
      "Training loss at step 150: 0.2070\n",
      "Training loss at step 175: 0.2124\n",
      "Training loss at step 200: 0.1346\n",
      "Training loss at step 225: 0.2008\n",
      "Training loss at step 250: 0.2317\n",
      "Training loss at step 275: 0.2871\n",
      "Training loss at step 300: 0.2170\n",
      "Training loss at step 325: 0.1325\n",
      "Training loss at step 350: 0.2591\n",
      "Training accuracy: 0.9322 Validation accuracy: 0.9046 Time taken: 124.27s\n",
      "Epoch 177/200\n",
      "Training loss at step 0: 0.1870\n",
      "Training loss at step 25: 0.3190\n",
      "Training loss at step 50: 0.2019\n",
      "Training loss at step 75: 0.1981\n",
      "Training loss at step 100: 0.1400\n",
      "Training loss at step 125: 0.1861\n",
      "Training loss at step 150: 0.2005\n",
      "Training loss at step 175: 0.1959\n",
      "Training loss at step 200: 0.1843\n",
      "Training loss at step 225: 0.1726\n",
      "Training loss at step 250: 0.1237\n",
      "Training loss at step 275: 0.1661\n",
      "Training loss at step 300: 0.1963\n",
      "Training loss at step 325: 0.2094\n",
      "Training loss at step 350: 0.1954\n",
      "Training accuracy: 0.9307 Validation accuracy: 0.9040 Time taken: 124.01s\n",
      "Epoch 178/200\n",
      "Training loss at step 0: 0.2737\n",
      "Training loss at step 25: 0.1828\n",
      "Training loss at step 50: 0.2219\n",
      "Training loss at step 75: 0.2629\n",
      "Training loss at step 100: 0.2295\n",
      "Training loss at step 125: 0.1912\n",
      "Training loss at step 150: 0.1007\n",
      "Training loss at step 175: 0.1533\n",
      "Training loss at step 200: 0.2212\n",
      "Training loss at step 225: 0.3028\n",
      "Training loss at step 250: 0.2116\n",
      "Training loss at step 275: 0.1928\n",
      "Training loss at step 300: 0.2084\n",
      "Training loss at step 325: 0.1625\n",
      "Training loss at step 350: 0.2371\n",
      "Training accuracy: 0.9309 Validation accuracy: 0.9042 Time taken: 124.17s\n",
      "Epoch 179/200\n",
      "Training loss at step 0: 0.2819\n",
      "Training loss at step 25: 0.2418\n",
      "Training loss at step 50: 0.2148\n",
      "Training loss at step 75: 0.1667\n",
      "Training loss at step 100: 0.1939\n",
      "Training loss at step 125: 0.1164\n",
      "Training loss at step 150: 0.1376\n",
      "Training loss at step 175: 0.1744\n",
      "Training loss at step 200: 0.2364\n",
      "Training loss at step 225: 0.2111\n",
      "Training loss at step 250: 0.2154\n",
      "Training loss at step 275: 0.2071\n",
      "Training loss at step 300: 0.2039\n",
      "Training loss at step 325: 0.1725\n",
      "Training loss at step 350: 0.1571\n",
      "Training accuracy: 0.9294 Validation accuracy: 0.9050 Time taken: 123.91s\n",
      "Epoch 180/200\n",
      "Training loss at step 0: 0.2482\n",
      "Training loss at step 25: 0.1639\n",
      "Training loss at step 50: 0.1956\n",
      "Training loss at step 75: 0.1462\n",
      "Training loss at step 100: 0.2452\n",
      "Training loss at step 125: 0.1884\n",
      "Training loss at step 150: 0.2159\n",
      "Training loss at step 175: 0.1797\n",
      "Training loss at step 200: 0.2084\n",
      "Training loss at step 225: 0.1784\n",
      "Training loss at step 250: 0.2364\n",
      "Training loss at step 275: 0.1259\n",
      "Training loss at step 300: 0.2184\n",
      "Training loss at step 325: 0.2153\n",
      "Training loss at step 350: 0.3023\n",
      "Training accuracy: 0.9300 Validation accuracy: 0.9048 Time taken: 124.01s\n",
      "Epoch 181/200\n",
      "Training loss at step 0: 0.2451\n",
      "Training loss at step 25: 0.1767\n",
      "Training loss at step 50: 0.2483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 75: 0.2003\n",
      "Training loss at step 100: 0.1740\n",
      "Training loss at step 125: 0.1206\n",
      "Training loss at step 150: 0.1532\n",
      "Training loss at step 175: 0.2467\n",
      "Training loss at step 200: 0.1424\n",
      "Training loss at step 225: 0.1999\n",
      "Training loss at step 250: 0.1651\n",
      "Training loss at step 275: 0.2001\n",
      "Training loss at step 300: 0.3165\n",
      "Training loss at step 325: 0.2642\n",
      "Training loss at step 350: 0.3807\n",
      "Training accuracy: 0.9306 Validation accuracy: 0.9034 Time taken: 122.82s\n",
      "Epoch 182/200\n",
      "Training loss at step 0: 0.1928\n",
      "Training loss at step 25: 0.1783\n",
      "Training loss at step 50: 0.1405\n",
      "Training loss at step 75: 0.2273\n",
      "Training loss at step 100: 0.1850\n",
      "Training loss at step 125: 0.2538\n",
      "Training loss at step 150: 0.1079\n",
      "Training loss at step 175: 0.1790\n",
      "Training loss at step 200: 0.1610\n",
      "Training loss at step 225: 0.2034\n",
      "Training loss at step 250: 0.1678\n",
      "Training loss at step 275: 0.4704\n",
      "Training loss at step 300: 0.1599\n",
      "Training loss at step 325: 0.3094\n",
      "Training loss at step 350: 0.1433\n",
      "Training accuracy: 0.9316 Validation accuracy: 0.9042 Time taken: 124.18s\n",
      "Epoch 183/200\n",
      "Training loss at step 0: 0.2259\n",
      "Training loss at step 25: 0.3384\n",
      "Training loss at step 50: 0.1126\n",
      "Training loss at step 75: 0.1962\n",
      "Training loss at step 100: 0.2317\n",
      "Training loss at step 125: 0.2251\n",
      "Training loss at step 150: 0.1049\n",
      "Training loss at step 175: 0.3211\n",
      "Training loss at step 200: 0.2093\n",
      "Training loss at step 225: 0.2347\n",
      "Training loss at step 250: 0.2831\n",
      "Training loss at step 275: 0.1311\n",
      "Training loss at step 300: 0.1774\n",
      "Training loss at step 325: 0.1887\n",
      "Training loss at step 350: 0.1665\n",
      "Training accuracy: 0.9312 Validation accuracy: 0.9052 Time taken: 123.39s\n",
      "Epoch 184/200\n",
      "Training loss at step 0: 0.2309\n",
      "Training loss at step 25: 0.1321\n",
      "Training loss at step 50: 0.1981\n",
      "Training loss at step 75: 0.2311\n",
      "Training loss at step 100: 0.1738\n",
      "Training loss at step 125: 0.1270\n",
      "Training loss at step 150: 0.1688\n",
      "Training loss at step 175: 0.2113\n",
      "Training loss at step 200: 0.1747\n",
      "Training loss at step 225: 0.2434\n",
      "Training loss at step 250: 0.1829\n",
      "Training loss at step 275: 0.2371\n",
      "Training loss at step 300: 0.1279\n",
      "Training loss at step 325: 0.2268\n",
      "Training loss at step 350: 0.1253\n",
      "Training accuracy: 0.9299 Validation accuracy: 0.9040 Time taken: 124.79s\n",
      "Epoch 185/200\n",
      "Training loss at step 0: 0.2049\n",
      "Training loss at step 25: 0.2147\n",
      "Training loss at step 50: 0.2382\n",
      "Training loss at step 75: 0.2529\n",
      "Training loss at step 100: 0.2084\n",
      "Training loss at step 125: 0.2178\n",
      "Training loss at step 150: 0.2210\n",
      "Training loss at step 175: 0.1885\n",
      "Training loss at step 200: 0.3074\n",
      "Training loss at step 225: 0.1922\n",
      "Training loss at step 250: 0.1149\n",
      "Training loss at step 275: 0.2481\n",
      "Training loss at step 300: 0.1658\n",
      "Training loss at step 325: 0.2664\n",
      "Training loss at step 350: 0.1907\n",
      "Training accuracy: 0.9286 Validation accuracy: 0.9034 Time taken: 123.91s\n",
      "Epoch 186/200\n",
      "Training loss at step 0: 0.3341\n",
      "Training loss at step 25: 0.2099\n",
      "Training loss at step 50: 0.1919\n",
      "Training loss at step 75: 0.2731\n",
      "Training loss at step 100: 0.2037\n",
      "Training loss at step 125: 0.3006\n",
      "Training loss at step 150: 0.1449\n",
      "Training loss at step 175: 0.2248\n",
      "Training loss at step 200: 0.2816\n",
      "Training loss at step 225: 0.0875\n",
      "Training loss at step 250: 0.1423\n",
      "Training loss at step 275: 0.1919\n",
      "Training loss at step 300: 0.1780\n",
      "Training loss at step 325: 0.2525\n",
      "Training loss at step 350: 0.1572\n",
      "Training accuracy: 0.9304 Validation accuracy: 0.9060 Time taken: 122.97s\n",
      "Epoch 187/200\n",
      "Training loss at step 0: 0.2165\n",
      "Training loss at step 25: 0.2311\n",
      "Training loss at step 50: 0.1608\n",
      "Training loss at step 75: 0.1121\n",
      "Training loss at step 100: 0.3238\n",
      "Training loss at step 125: 0.1863\n",
      "Training loss at step 150: 0.1784\n",
      "Training loss at step 175: 0.1634\n",
      "Training loss at step 200: 0.1868\n",
      "Training loss at step 225: 0.2132\n",
      "Training loss at step 250: 0.1118\n",
      "Training loss at step 275: 0.1240\n",
      "Training loss at step 300: 0.2241\n",
      "Training loss at step 325: 0.2023\n",
      "Training loss at step 350: 0.1949\n",
      "Training accuracy: 0.9331 Validation accuracy: 0.9048 Time taken: 123.82s\n",
      "Epoch 188/200\n",
      "Training loss at step 0: 0.2307\n",
      "Training loss at step 25: 0.0956\n",
      "Training loss at step 50: 0.1704\n",
      "Training loss at step 75: 0.1699\n",
      "Training loss at step 100: 0.2958\n",
      "Training loss at step 125: 0.1677\n",
      "Training loss at step 150: 0.1761\n",
      "Training loss at step 175: 0.2669\n",
      "Training loss at step 200: 0.1605\n",
      "Training loss at step 225: 0.0947\n",
      "Training loss at step 250: 0.1999\n",
      "Training loss at step 275: 0.1623\n",
      "Training loss at step 300: 0.2173\n",
      "Training loss at step 325: 0.1795\n",
      "Training loss at step 350: 0.2284\n",
      "Training accuracy: 0.9316 Validation accuracy: 0.9032 Time taken: 123.83s\n",
      "Epoch 189/200\n",
      "Training loss at step 0: 0.3385\n",
      "Training loss at step 25: 0.2106\n",
      "Training loss at step 50: 0.1915\n",
      "Training loss at step 75: 0.2360\n",
      "Training loss at step 100: 0.1599\n",
      "Training loss at step 125: 0.2561\n",
      "Training loss at step 150: 0.1656\n",
      "Training loss at step 175: 0.1644\n",
      "Training loss at step 200: 0.2256\n",
      "Training loss at step 225: 0.1292\n",
      "Training loss at step 250: 0.1296\n",
      "Training loss at step 275: 0.1925\n",
      "Training loss at step 300: 0.1406\n",
      "Training loss at step 325: 0.1628\n",
      "Training loss at step 350: 0.1968\n",
      "Training accuracy: 0.9324 Validation accuracy: 0.9036 Time taken: 124.08s\n",
      "Epoch 190/200\n",
      "Training loss at step 0: 0.1173\n",
      "Training loss at step 25: 0.2231\n",
      "Training loss at step 50: 0.2204\n",
      "Training loss at step 75: 0.2201\n",
      "Training loss at step 100: 0.1540\n",
      "Training loss at step 125: 0.1472\n",
      "Training loss at step 150: 0.1309\n",
      "Training loss at step 175: 0.2340\n",
      "Training loss at step 200: 0.2991\n",
      "Training loss at step 225: 0.3164\n",
      "Training loss at step 250: 0.1421\n",
      "Training loss at step 275: 0.1617\n",
      "Training loss at step 300: 0.1990\n",
      "Training loss at step 325: 0.1672\n",
      "Training loss at step 350: 0.2261\n",
      "Training accuracy: 0.9316 Validation accuracy: 0.9044 Time taken: 123.57s\n",
      "Epoch 191/200\n",
      "Training loss at step 0: 0.1697\n",
      "Training loss at step 25: 0.1410\n",
      "Training loss at step 50: 0.2472\n",
      "Training loss at step 75: 0.1283\n",
      "Training loss at step 100: 0.1841\n",
      "Training loss at step 125: 0.1590\n",
      "Training loss at step 150: 0.2612\n",
      "Training loss at step 175: 0.2034\n",
      "Training loss at step 200: 0.1392\n",
      "Training loss at step 225: 0.1481\n",
      "Training loss at step 250: 0.2622\n",
      "Training loss at step 275: 0.1577\n",
      "Training loss at step 300: 0.3119\n",
      "Training loss at step 325: 0.2472\n",
      "Training loss at step 350: 0.1252\n",
      "Training accuracy: 0.9294 Validation accuracy: 0.9046 Time taken: 124.45s\n",
      "Epoch 192/200\n",
      "Training loss at step 0: 0.1755\n",
      "Training loss at step 25: 0.1537\n",
      "Training loss at step 50: 0.2056\n",
      "Training loss at step 75: 0.2245\n",
      "Training loss at step 100: 0.2142\n",
      "Training loss at step 125: 0.1853\n",
      "Training loss at step 150: 0.3786\n",
      "Training loss at step 175: 0.2117\n",
      "Training loss at step 200: 0.1657\n",
      "Training loss at step 225: 0.1707\n",
      "Training loss at step 250: 0.1979\n",
      "Training loss at step 275: 0.2224\n",
      "Training loss at step 300: 0.1871\n",
      "Training loss at step 325: 0.1978\n",
      "Training loss at step 350: 0.1866\n",
      "Training accuracy: 0.9315 Validation accuracy: 0.9058 Time taken: 123.72s\n",
      "Epoch 193/200\n",
      "Training loss at step 0: 0.1984\n",
      "Training loss at step 25: 0.1423\n",
      "Training loss at step 50: 0.1814\n",
      "Training loss at step 75: 0.1516\n",
      "Training loss at step 100: 0.2094\n",
      "Training loss at step 125: 0.1973\n",
      "Training loss at step 150: 0.2772\n",
      "Training loss at step 175: 0.1717\n",
      "Training loss at step 200: 0.1544\n",
      "Training loss at step 225: 0.2089\n",
      "Training loss at step 250: 0.1634\n",
      "Training loss at step 275: 0.2243\n",
      "Training loss at step 300: 0.1857\n",
      "Training loss at step 325: 0.1441\n",
      "Training loss at step 350: 0.1034\n",
      "Training accuracy: 0.9315 Validation accuracy: 0.9040 Time taken: 123.82s\n",
      "Epoch 194/200\n",
      "Training loss at step 0: 0.1971\n",
      "Training loss at step 25: 0.2018\n",
      "Training loss at step 50: 0.1502\n",
      "Training loss at step 75: 0.1617\n",
      "Training loss at step 100: 0.1994\n",
      "Training loss at step 125: 0.1103\n",
      "Training loss at step 150: 0.1180\n",
      "Training loss at step 175: 0.1757\n",
      "Training loss at step 200: 0.1749\n",
      "Training loss at step 225: 0.1784\n",
      "Training loss at step 250: 0.2605\n",
      "Training loss at step 275: 0.2206\n",
      "Training loss at step 300: 0.1896\n",
      "Training loss at step 325: 0.2524\n",
      "Training loss at step 350: 0.1959\n",
      "Training accuracy: 0.9317 Validation accuracy: 0.9050 Time taken: 123.57s\n",
      "Epoch 195/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 0: 0.1481\n",
      "Training loss at step 25: 0.1522\n",
      "Training loss at step 50: 0.1843\n",
      "Training loss at step 75: 0.1958\n",
      "Training loss at step 100: 0.2150\n",
      "Training loss at step 125: 0.2805\n",
      "Training loss at step 150: 0.1846\n",
      "Training loss at step 175: 0.1973\n",
      "Training loss at step 200: 0.1631\n",
      "Training loss at step 225: 0.2143\n",
      "Training loss at step 250: 0.1023\n",
      "Training loss at step 275: 0.1628\n",
      "Training loss at step 300: 0.2146\n",
      "Training loss at step 325: 0.2324\n",
      "Training loss at step 350: 0.2088\n",
      "Training accuracy: 0.9311 Validation accuracy: 0.9046 Time taken: 123.98s\n",
      "Epoch 196/200\n",
      "Training loss at step 0: 0.2120\n",
      "Training loss at step 25: 0.2208\n",
      "Training loss at step 50: 0.2005\n",
      "Training loss at step 75: 0.2160\n",
      "Training loss at step 100: 0.1932\n",
      "Training loss at step 125: 0.3035\n",
      "Training loss at step 150: 0.2412\n",
      "Training loss at step 175: 0.2233\n",
      "Training loss at step 200: 0.1878\n",
      "Training loss at step 225: 0.2819\n",
      "Training loss at step 250: 0.1573\n",
      "Training loss at step 275: 0.2567\n",
      "Training loss at step 300: 0.1745\n",
      "Training loss at step 325: 0.1432\n",
      "Training loss at step 350: 0.2678\n",
      "Training accuracy: 0.9322 Validation accuracy: 0.9052 Time taken: 123.36s\n",
      "Epoch 197/200\n",
      "Training loss at step 0: 0.2371\n",
      "Training loss at step 25: 0.3329\n",
      "Training loss at step 50: 0.2219\n",
      "Training loss at step 75: 0.2295\n",
      "Training loss at step 100: 0.1688\n",
      "Training loss at step 125: 0.2765\n",
      "Training loss at step 150: 0.1651\n",
      "Training loss at step 175: 0.3006\n",
      "Training loss at step 200: 0.2974\n",
      "Training loss at step 225: 0.2096\n",
      "Training loss at step 250: 0.2460\n",
      "Training loss at step 275: 0.1524\n",
      "Training loss at step 300: 0.1771\n",
      "Training loss at step 325: 0.1298\n",
      "Training loss at step 350: 0.1413\n",
      "Training accuracy: 0.9308 Validation accuracy: 0.9038 Time taken: 123.87s\n",
      "Epoch 198/200\n",
      "Training loss at step 0: 0.2335\n",
      "Training loss at step 25: 0.2004\n",
      "Training loss at step 50: 0.0779\n",
      "Training loss at step 75: 0.2148\n",
      "Training loss at step 100: 0.2787\n",
      "Training loss at step 125: 0.2502\n",
      "Training loss at step 150: 0.2106\n",
      "Training loss at step 175: 0.2218\n",
      "Training loss at step 200: 0.1954\n",
      "Training loss at step 225: 0.2386\n",
      "Training loss at step 250: 0.1983\n",
      "Training loss at step 275: 0.3183\n",
      "Training loss at step 300: 0.2520\n",
      "Training loss at step 325: 0.1409\n",
      "Training loss at step 350: 0.0955\n",
      "Training accuracy: 0.9326 Validation accuracy: 0.9048 Time taken: 123.20s\n",
      "Epoch 199/200\n",
      "Training loss at step 0: 0.1183\n",
      "Training loss at step 25: 0.2052\n",
      "Training loss at step 50: 0.1943\n",
      "Training loss at step 75: 0.1396\n",
      "Training loss at step 100: 0.3128\n",
      "Training loss at step 125: 0.1958\n",
      "Training loss at step 150: 0.2051\n",
      "Training loss at step 175: 0.1997\n",
      "Training loss at step 200: 0.1977\n",
      "Training loss at step 225: 0.3298\n",
      "Training loss at step 250: 0.3065\n",
      "Training loss at step 275: 0.2519\n",
      "Training loss at step 300: 0.0894\n",
      "Training loss at step 325: 0.1583\n",
      "Training loss at step 350: 0.1560\n",
      "Training accuracy: 0.9312 Validation accuracy: 0.9040 Time taken: 123.59s\n",
      "Epoch 200/200\n",
      "Training loss at step 0: 0.2366\n",
      "Training loss at step 25: 0.2164\n",
      "Training loss at step 50: 0.1424\n",
      "Training loss at step 75: 0.1622\n",
      "Training loss at step 100: 0.2327\n",
      "Training loss at step 125: 0.2527\n",
      "Training loss at step 150: 0.2188\n",
      "Training loss at step 175: 0.1848\n",
      "Training loss at step 200: 0.1800\n",
      "Training loss at step 225: 0.2497\n",
      "Training loss at step 250: 0.1979\n",
      "Training loss at step 275: 0.2110\n",
      "Training loss at step 300: 0.2633\n",
      "Training loss at step 325: 0.2447\n",
      "Training loss at step 350: 0.1632\n",
      "Training accuracy: 0.9324 Validation accuracy: 0.9046 Time taken: 122.85s\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1,nesterov=True)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "history = [[],[],[]]\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch %d/%d\" % (epoch+1,epochs))\n",
    "    if (epoch==100) | (epoch==150):\n",
    "        optimizer.learning_rate = optimizer.learning_rate/10\n",
    "    \n",
    "    start_time = time.time()\n",
    "    step = 0\n",
    "    for x_batch_train, y_batch_train in train_data:\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "        if step % 25 == 0:\n",
    "            print(\n",
    "                \"Training loss at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "        step += 1\n",
    "        if step > len(x_train)/batch_size:\n",
    "            break\n",
    "\n",
    "    history[0].append(loss_value.numpy())\n",
    "    train_acc = train_acc_metric.result()\n",
    "    train_acc_metric.reset_states()\n",
    "    \n",
    "    step = 0\n",
    "    for x_batch_val, y_batch_val in validation_data:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "        step += 1\n",
    "        if step > len(x_val)/batch_size:\n",
    "            break\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    history[1].append(train_acc.numpy())\n",
    "    history[2].append(val_acc.numpy())\n",
    "    print(\"Training accuracy: %.4f\" % (float(train_acc),)\n",
    "          ,\"Validation accuracy: %.4f\" % (float(val_acc),),\"Time taken: %.2fs\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea9b850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log = np.array(history)\n",
    "# np.save(\"./Logs/ResNet110_cifar10\",log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9967c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Logs/ResNet110_cifar10.npy', 'rb') as f:\n",
    "     log = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97d0d541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f648bfe6390>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(1-log[1],label='training error')\n",
    "plt.plot(1-log[2],label='validation error')\n",
    "plt.ylim(0,0.4)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2fdf7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
