{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "environmental-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "personalized-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data(label_mode='fine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "dated-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train-np.mean(x_train,axis=0)\n",
    "x_test = x_test - np.mean(x_test,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "informal-canyon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "rising-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Conv2D, GlobalAveragePooling2D, BatchNormalization\n",
    "input_shape = (32,32,3)\n",
    "\"\"\"\n",
    "For training on cifar10/cifar100 data\n",
    "create a plain network model, 6n+2 layers,\n",
    "\"\"\"\n",
    "def create_model(input_shape,n):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=(3, 3),input_shape=input_shape,activation='relu'\n",
    "             ,kernel_initializer=tf.keras.initializers.HeNormal(),padding=\"same\",use_bias=True))\n",
    "    for i in range(2*n):\n",
    "        model.add(Conv2D(filters=16,kernel_size=(3, 3),activation='relu',padding=\"same\"\n",
    "                     ,use_bias=True,kernel_initializer=tf.keras.initializers.HeNormal()))\n",
    "    model.add(Conv2D(filters=32,kernel_size=(3, 3),activation='relu',strides=(2,2),padding=\"same\"\n",
    "             ,use_bias=True,kernel_initializer=tf.keras.initializers.HeNormal()))\n",
    "    for i in range(2*n-1):\n",
    "        model.add(Conv2D(filters=32,kernel_size=(3, 3),activation='relu',padding=\"same\"\n",
    "                     ,use_bias=True,kernel_initializer=tf.keras.initializers.HeNormal()))\n",
    "    model.add(Conv2D(filters=64,kernel_size=(3, 3),activation='relu',strides=(2,2),padding=\"same\"\n",
    "             ,use_bias=True,kernel_initializer=tf.keras.initializers.HeNormal()))\n",
    "    for i in range(2*n-1):\n",
    "        model.add(Conv2D(filters=64,kernel_size=(3, 3),activation='relu',padding=\"same\"\n",
    "                     ,use_bias=True,kernel_initializer=tf.keras.initializers.HeNormal()))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(100,activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "personalized-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(input_shape,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "communist-hypothetical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 100), dtype=float32, numpy=\n",
       "array([[6.38601274e-16, 5.94529851e-14, 1.84309363e-24, 7.04296315e-17,\n",
       "        1.30081588e-17, 2.81793123e-15, 8.36956449e-10, 8.45094827e-12,\n",
       "        4.22094178e-25, 1.04983708e-12, 4.03955694e-19, 5.96330494e-31,\n",
       "        2.21559132e-10, 4.21056136e-21, 2.77805837e-28, 6.35065589e-19,\n",
       "        2.12544687e-23, 2.31435912e-10, 1.03491773e-20, 1.82689159e-21,\n",
       "        8.94365657e-15, 5.46056689e-13, 2.83260422e-04, 5.31081717e-16,\n",
       "        3.31741052e-22, 2.34486111e-11, 3.90192241e-07, 8.01006765e-16,\n",
       "        1.78085960e-20, 5.48781533e-14, 1.30555221e-19, 1.87901517e-09,\n",
       "        8.75484238e-19, 1.90373543e-22, 2.65624344e-16, 3.90123345e-13,\n",
       "        5.64213999e-21, 9.84520576e-20, 9.83486567e-16, 1.91014322e-23,\n",
       "        1.94508791e-20, 4.17352649e-13, 4.40963192e-15, 7.90996810e-28,\n",
       "        8.31963306e-22, 1.49030500e-04, 3.82558343e-26, 6.03917705e-29,\n",
       "        2.48749605e-17, 1.27221751e-21, 9.54995217e-19, 8.73435808e-27,\n",
       "        1.00831697e-13, 1.48142307e-34, 2.50519747e-07, 3.64935419e-18,\n",
       "        5.63634411e-24, 1.19686986e-16, 3.08366182e-22, 5.39485624e-18,\n",
       "        1.02239249e-20, 1.61682648e-14, 2.78125357e-18, 4.45769121e-18,\n",
       "        7.85088415e-24, 1.83450667e-14, 2.05385909e-19, 9.54068838e-11,\n",
       "        2.39617337e-14, 1.21457379e-22, 9.99565065e-01, 4.63424299e-17,\n",
       "        1.47381608e-12, 1.97742438e-26, 2.33859709e-24, 4.47373312e-13,\n",
       "        9.15693692e-15, 3.10899751e-22, 3.85929112e-21, 2.19909523e-07,\n",
       "        1.17754464e-18, 2.60890230e-25, 1.41849685e-15, 2.63224225e-19,\n",
       "        1.89561778e-16, 1.92409254e-20, 1.87368801e-11, 9.73442388e-31,\n",
       "        4.45160091e-21, 4.52004808e-21, 8.63589760e-26, 1.85544150e-06,\n",
       "        5.81425678e-26, 1.01864169e-16, 6.62613575e-22, 2.56780856e-25,\n",
       "        2.14715892e-21, 4.64472211e-16, 3.96380218e-17, 6.20544746e-23],\n",
       "       [5.41757395e-09, 7.91799426e-09, 5.06169525e-15, 3.69787451e-10,\n",
       "        2.87224810e-11, 1.56386926e-09, 1.74378317e-06, 2.28672633e-07,\n",
       "        1.13462477e-15, 1.50310164e-08, 2.02071054e-10, 7.58317051e-19,\n",
       "        3.55128736e-06, 1.58555108e-13, 1.84105771e-17, 7.09046233e-12,\n",
       "        1.30064416e-13, 7.68597090e-07, 1.49484944e-13, 3.48241076e-13,\n",
       "        2.38213502e-08, 2.53300556e-07, 1.15021784e-03, 6.85889623e-10,\n",
       "        7.18332186e-13, 8.74185417e-08, 3.81985679e-04, 2.19816373e-10,\n",
       "        1.19228209e-13, 5.72838799e-09, 8.23939961e-12, 5.29680983e-06,\n",
       "        3.13223644e-12, 1.90028576e-13, 1.97062144e-09, 2.90048359e-08,\n",
       "        1.50155916e-13, 1.46620563e-12, 4.90376795e-10, 6.39698730e-14,\n",
       "        9.43639372e-13, 1.26968160e-07, 1.59819424e-09, 1.29047306e-17,\n",
       "        7.19857049e-14, 4.93253465e-04, 2.49659838e-16, 1.53064095e-16,\n",
       "        2.15676185e-10, 5.60876866e-13, 3.57362816e-12, 9.34816530e-17,\n",
       "        1.49994843e-08, 1.97349930e-20, 3.63207641e-06, 4.66140538e-12,\n",
       "        4.13866481e-15, 7.51521401e-10, 2.74648095e-14, 5.53439725e-11,\n",
       "        3.98881535e-12, 3.90640631e-09, 3.57383463e-11, 2.81467454e-11,\n",
       "        1.73226888e-15, 4.62113370e-09, 5.06834028e-12, 8.16698673e-07,\n",
       "        6.70531941e-09, 8.20282037e-15, 9.96170938e-01, 9.10977613e-11,\n",
       "        3.00579899e-07, 8.68958607e-16, 2.42247484e-15, 1.32968509e-07,\n",
       "        1.86032789e-09, 8.86684863e-14, 5.49610108e-13, 1.32638472e-03,\n",
       "        9.36442850e-12, 2.88109654e-16, 4.70622985e-10, 6.16879256e-11,\n",
       "        8.55964188e-10, 5.54743317e-13, 1.44767239e-06, 4.09008062e-19,\n",
       "        7.40603163e-13, 5.00295877e-13, 6.50239242e-16, 4.58688883e-04,\n",
       "        9.91624515e-17, 1.13397861e-10, 1.48444476e-13, 6.04335265e-15,\n",
       "        4.11797576e-13, 4.21841978e-10, 1.30762024e-11, 1.02006592e-13]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_train[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-police",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aggressive-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "rubber-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x=x_train, \n",
    "#           y=y_train,\n",
    "#           batch_size=128,\n",
    "#           epochs=10, \n",
    "#           validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "japanese-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, GlobalAveragePooling2D, BatchNormalization, ReLU\n",
    "\n",
    "class ResNet(Model):\n",
    "\n",
    "    def __init__(self, num_block, input_shape, output_size=100):\n",
    "        '''\n",
    "        For training the cifar10/cifar100 data in the ResNet paper\n",
    "        input_shape: The size of the input. (img_len, img_len, channel_num).\n",
    "        output_size: The size of the output. It should be equal to the number of classes.\n",
    "        '''\n",
    "        super(ResNet, self).__init__()\n",
    "        #############################################################\n",
    "        # TODO: Define layers for your custom LeNet network         \n",
    "        # Hint: Try adding additional convolution and avgpool layers\n",
    "        #############################################################\n",
    "        self.num_block = num_block\n",
    "        self.layer = dict()\n",
    "        self.layer['conv1'] = Conv2D(filters=16,kernel_size=(3, 3),input_shape=input_shape,activation='relu'\n",
    "                                ,kernel_initializer=tf.keras.initializers.HeNormal(),padding=\"same\",use_bias=True)\n",
    "\n",
    "        for i in range(2*num_block):\n",
    "            self.layer['conv1_%d'%(i+1)] = Conv2D(filters=16,kernel_size=(3, 3),activation='relu',padding=\"same\"\n",
    "                                    ,use_bias=True,kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "\n",
    "        self.layer['conv2_1'] = Conv2D(filters=32,kernel_size=(3, 3),activation='relu',strides=(2,2),padding=\"same\"\n",
    "             ,use_bias=True,kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "        for i in range(2*num_block-1):\n",
    "            self.layer['conv2_%d'%(i+2)] = Conv2D(filters=32,kernel_size=(3, 3),activation='relu',padding=\"same\"\n",
    "                     ,use_bias=True,kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "        \n",
    "        self.layer['conv3_1'] = Conv2D(filters=64,kernel_size=(3, 3),activation='relu',strides=(2,2),padding=\"same\"\n",
    "             ,use_bias=True,kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "        for i in range(2*num_block-1):\n",
    "            self.layer['conv3_%d'%(i+2)] = Conv2D(filters=64,kernel_size=(3, 3),activation='relu',padding=\"same\"\n",
    "                     ,use_bias=True,kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "\n",
    "        \n",
    "        self.layer['avgpool'] = GlobalAveragePooling2D()\n",
    "        self.layer['bn'] = BatchNormalization()\n",
    "        self.layer['fc'] = Dense(100,activation='softmax')\n",
    "    \n",
    "        #############################################################\n",
    "        #                          END TODO                         #                                              \n",
    "        #############################################################\n",
    "\n",
    "    \n",
    "    def call(self, x):\n",
    "        '''\n",
    "        x: input to LeNet model.\n",
    "        '''\n",
    "        #call function returns forward pass output of the network\n",
    "        #############################################################\n",
    "        # TODO: Implement forward pass for custom network defined \n",
    "        # in __init__ and return network output\n",
    "        #############################################################\n",
    "        num_block = self.num_block\n",
    "        x = self.layer['conv1'](x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x_prev = x\n",
    "        \n",
    "        for i in range(2*num_block):\n",
    "            if i%2 == 0:\n",
    "                x = self.layer['conv1_%d'%(i+1)](x)\n",
    "                x = BatchNormalization()(x)\n",
    "            else:\n",
    "                x = ReLU()(self.layer['conv1_%d'%(i+1)](x) + x_prev)\n",
    "                x = BatchNormalization()(x)\n",
    "                x_prev = x\n",
    "        \n",
    "\n",
    "\n",
    "        for i in range(2*num_block):\n",
    "            if i%2 == 0:\n",
    "                x = self.layer['conv2_%d'%(i+1)](x)\n",
    "                x = BatchNormalization()(x)\n",
    "            else:\n",
    "                if x.shape == x_prev.shape:\n",
    "                    x =  ReLU()(self.layer['conv2_%d'%(i+1)](x) + x_prev)\n",
    "                    x = BatchNormalization()(x)\n",
    "                    x_prev = x\n",
    "                else:\n",
    "                    x_prev = Conv2D(filters=32,kernel_size=(1, 1),strides=(2, 2)\n",
    "                            ,use_bias=False,kernel_initializer=tf.keras.initializers.Ones())(x_prev)/16\n",
    "                    x =  ReLU()(self.layer['conv2_%d'%(i+1)](x) + x_prev)\n",
    "                    x = BatchNormalization()(x)\n",
    "                    x_prev = x\n",
    "\n",
    "        for i in range(2*num_block):\n",
    "            if i%2 == 0:\n",
    "                x = self.layer['conv3_%d'%(i+1)](x)\n",
    "                x = BatchNormalization()(x)\n",
    "            else:\n",
    "                if x.shape == x_prev.shape:\n",
    "                    x =  ReLU()(self.layer['conv3_%d'%(i+1)](x) + x_prev)\n",
    "                    x = BatchNormalization()(x)\n",
    "                    x_prev = x\n",
    "                else:\n",
    "                    x_prev = Conv2D(filters=64,kernel_size=(1, 1),strides=(2, 2)\n",
    "                            ,use_bias=False,kernel_initializer=tf.keras.initializers.Ones())(x_prev)/32\n",
    "                    x =  ReLU()(self.layer['conv3_%d'%(i+1)](x) + x_prev)\n",
    "                    x = BatchNormalization()(x)\n",
    "                    x_prev = x\n",
    "\n",
    "        x = self.layer['avgpool'](x)\n",
    "        x = self.layer['bn'](x)\n",
    "        out = self.layer['fc'](x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "approximate-company",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Training loss at step 0: 5.2383\n",
      "Training accuracy: 0.1220 Validation accuracy: 0.0087 Time taken: 145.43s\n",
      "Epoch 2/5\n",
      "Training loss at step 0: 3.3683\n",
      "Training accuracy: 0.2279 Validation accuracy: 0.0100 Time taken: 142.66s\n",
      "Epoch 3/5\n",
      "Training loss at step 0: 2.9697\n",
      "Training accuracy: 0.2871 Validation accuracy: 0.0100 Time taken: 141.12s\n",
      "Epoch 4/5\n",
      "Training loss at step 0: 2.7919\n",
      "Training accuracy: 0.3289 Validation accuracy: 0.0100 Time taken: 131.99s\n",
      "Epoch 5/5\n",
      "Training loss at step 0: 2.6243\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nd/hzd0zcf9363clrvwfc13r9940000gn/T/ipykernel_92011/2464554694.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtrain_acc_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/deep learning/env/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1078\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1081\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/deep learning/env/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/deep learning/env/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/deep learning/env/lib/python3.8/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_FusedBatchNormV3Grad\u001b[0;34m(op, *grad)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegisterGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FusedBatchNormV3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_FusedBatchNormV3Grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_BaseFusedBatchNormGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/deep learning/env/lib/python3.8/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_BaseFusedBatchNormGrad\u001b[0;34m(op, version, *grad)\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reserve_space_3\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m     \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     \u001b[0mpop_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/deep learning/env/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mfused_batch_norm_grad_v3\u001b[0;34m(y_backprop, x, scale, reserve_space_1, reserve_space_2, reserve_space_3, epsilon, data_format, is_training, name)\u001b[0m\n\u001b[1;32m   3996\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3997\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3998\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3999\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FusedBatchNormGradV3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_backprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4000\u001b[0m         \u001b[0mreserve_space_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreserve_space_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreserve_space_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"epsilon\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "model = ResNet(num_block=1, input_shape=input_shape)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "batch_size = 128\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "history = [[],[],[]]\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch %d/%d\" % (epoch+1,epochs))\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "        if step % 400 == 0:\n",
    "            print(\n",
    "                \"Training loss at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "\n",
    "    history[0].append(loss_value)\n",
    "    train_acc = train_acc_metric.result()\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    history[1].append(train_acc)\n",
    "    history[2].append(val_acc)\n",
    "    print(\"Training accuracy: %.4f\" % (float(train_acc),)\n",
    "          ,\"Validation accuracy: %.4f\" % (float(val_acc),),\"Time taken: %.2fs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "hundred-living",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Conv2D(filters=16,kernel_size=(3, 3),input_shape=input_shape,activation='relu'\n",
    "                                ,kernel_initializer=tf.keras.initializers.HeNormal(),padding=\"same\",use_bias=True)(x)\n",
    "x_pre = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "changed-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Conv2D(filters=16,kernel_size=(3, 3),activation='relu',padding=\"same\"\n",
    "                                    ,use_bias=True,kernel_initializer=tf.keras.initializers.HeNormal())(x)\n",
    "x = Conv2D(filters=16,kernel_size=(3, 3),activation='relu',padding=\"same\"\n",
    "                                    ,use_bias=True,kernel_initializer=tf.keras.initializers.HeNormal())(x) + x_pre\n",
    "x =  ReLU()(x)\n",
    "x_pre = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "weird-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Conv2D(filters=32,kernel_size=(3, 3),activation='relu',strides=(2,2),padding=\"same\"\n",
    "             ,use_bias=True,kernel_initializer=tf.keras.initializers.HeNormal())(x)\n",
    "x_pre = Conv2D(filters=32,kernel_size=(1, 1),strides=(2, 2)\n",
    "                            ,use_bias=False,kernel_initializer=tf.keras.initializers.Ones())(x_pre)/16\n",
    "x = Conv2D(filters=32,kernel_size=(3, 3),activation='relu',padding=\"same\"\n",
    "                     ,use_bias=True,kernel_initializer=tf.keras.initializers.HeNormal())(x) + x_pre\n",
    "x =  ReLU()(x)\n",
    "x_pre = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "indie-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Conv2D(filters=64,kernel_size=(3, 3),activation='relu',strides=(2,2),padding=\"same\"\n",
    "             ,use_bias=True,kernel_initializer=tf.keras.initializers.HeNormal())(x)\n",
    "x_pre = Conv2D(filters=64,kernel_size=(1, 1),strides=(2, 2)\n",
    "                            ,use_bias=False,kernel_initializer=tf.keras.initializers.Ones())(x_pre)/32\n",
    "x = Conv2D(filters=64,kernel_size=(3, 3),activation='relu',padding=\"same\"\n",
    "                     ,use_bias=True,kernel_initializer=tf.keras.initializers.HeNormal())(x) + x_pre\n",
    "x =  ReLU()(x)\n",
    "x_pre = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "incorrect-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = GlobalAveragePooling2D()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "international-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(100,activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "decent-console",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 100), dtype=float32, numpy=\n",
       "array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        3.2832270e-07, 0.0000000e+00, 0.0000000e+00, 3.9675338e-03,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        9.9603206e-01, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        6.7386478e-11, 0.0000000e+00, 0.0000000e+00, 8.1893960e-03,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        9.9181056e-01, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 6.3687607e-35, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 7.5847671e-33,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.3695788e-24,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 3.3427092e-21, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        9.4956232e-34, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        8.8881993e-01, 0.0000000e+00, 0.0000000e+00, 1.0045591e-01,\n",
       "        2.7188192e-28, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.0724231e-02, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 3.6736628e-27, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.9436832e-20,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 1.4081984e-13, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        3.2768300e-15, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 9.1919850e-31, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-white",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
