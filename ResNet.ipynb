{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "environmental-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utils.neuralnets.ResNet110 import ResNet110\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "personalized-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load data and standardize\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = (x_train - np.mean(x_train,axis=0))/np.std(x_train,axis=0)\n",
    "x_test = (x_test - np.mean(x_test,axis=0))/np.std(x_test,axis=0)\n",
    "##train validation split, 45000 for training and 5000 for validation\n",
    "np.random.seed(42)\n",
    "mask_val = np.random.choice(50000,5000,replace=False)\n",
    "mask_train = np.array([i for i in range(50000) if i not in mask_val])\n",
    "x_val, y_val = x_train[mask_val], y_train[mask_val]\n",
    "x_train, y_train = x_train[mask_train], y_train[mask_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "streaming-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "##data augmentation\n",
    "##augmented with horizontal flip and random erasing\n",
    "def randomErasing(x, cut_size=16):\n",
    "    x = np.copy(x)\n",
    "    fill = x.mean()\n",
    "\n",
    "    h, w, _ = x.shape\n",
    "    top = np.random.randint(0 - cut_size // 2, h - cut_size)\n",
    "    left = np.random.randint(0 - cut_size // 2, w - cut_size)\n",
    "    bottom = top + cut_size\n",
    "    right = left + cut_size\n",
    "    if top < 0:\n",
    "        top = 0\n",
    "    if left < 0:\n",
    "        left = 0\n",
    "    x[top:bottom, left:right, :].fill(fill)\n",
    "    return x\n",
    "batch_size = 128\n",
    "img_datagen = ImageDataGenerator(horizontal_flip=True,preprocessing_function = randomErasing)\n",
    "train_data = img_datagen.flow(x_train,y_train,batch_size=batch_size)\n",
    "validation_data = datagen_for_test.flow(x_val, y_val, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "center-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create a ResNet110 model\n",
    "input_shape = x_train.shape[1:]\n",
    "num_class = 10\n",
    "model = ResNet110(input_shape=input_shape,num_class=num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-kazakhstan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "Training loss at step 0: 17.1206\n",
      "Training loss at step 25: 6.1403\n",
      "Training loss at step 50: 2.6946\n",
      "Training loss at step 75: 2.7706\n",
      "Training loss at step 100: 2.5718\n",
      "Training loss at step 125: 2.2261\n",
      "Training loss at step 150: 2.2935\n",
      "Training loss at step 175: 2.3775\n",
      "Training loss at step 200: 2.1292\n",
      "Training loss at step 225: 2.1434\n",
      "Training loss at step 250: 2.1994\n",
      "Training loss at step 275: 2.0656\n",
      "Training loss at step 300: 2.0139\n",
      "Training loss at step 325: 2.0517\n",
      "Training loss at step 350: 2.0764\n",
      "Training accuracy: 0.1773 Validation accuracy: 0.2278 Time taken: 1390.33s\n",
      "Epoch 2/200\n",
      "Training loss at step 0: 1.9938\n",
      "Training loss at step 25: 2.0927\n",
      "Training loss at step 50: 1.9621\n",
      "Training loss at step 75: 1.9845\n",
      "Training loss at step 100: 1.8682\n",
      "Training loss at step 125: 1.8642\n",
      "Training loss at step 150: 1.9370\n",
      "Training loss at step 175: 1.8850\n",
      "Training loss at step 200: 1.8585\n",
      "Training loss at step 225: 1.7437\n",
      "Training loss at step 250: 1.8139\n",
      "Training loss at step 275: 1.9565\n",
      "Training loss at step 300: 1.7829\n",
      "Training loss at step 325: 1.8206\n",
      "Training loss at step 350: 1.5769\n",
      "Training accuracy: 0.3106 Validation accuracy: 0.3358 Time taken: 1366.82s\n",
      "Epoch 3/200\n",
      "Training loss at step 0: 1.6997\n",
      "Training loss at step 25: 1.7459\n",
      "Training loss at step 50: 1.5363\n",
      "Training loss at step 75: 1.6837\n",
      "Training loss at step 100: 1.6287\n",
      "Training loss at step 125: 1.7377\n",
      "Training loss at step 150: 1.7384\n",
      "Training loss at step 175: 1.5826\n",
      "Training loss at step 200: 1.7284\n",
      "Training loss at step 225: 1.5707\n",
      "Training loss at step 250: 1.5311\n",
      "Training loss at step 275: 1.6693\n",
      "Training loss at step 300: 1.5871\n",
      "Training loss at step 325: 1.6489\n",
      "Training loss at step 350: 1.7486\n",
      "Training accuracy: 0.3842 Validation accuracy: 0.3584 Time taken: 1402.68s\n",
      "Epoch 4/200\n",
      "Training loss at step 0: 1.8015\n",
      "Training loss at step 25: 1.5882\n",
      "Training loss at step 50: 1.7033\n",
      "Training loss at step 75: 1.5905\n",
      "Training loss at step 100: 1.6716\n",
      "Training loss at step 125: 1.6926\n",
      "Training loss at step 150: 1.5504\n",
      "Training loss at step 175: 1.6007\n",
      "Training loss at step 200: 1.5622\n",
      "Training loss at step 225: 1.6471\n",
      "Training loss at step 250: 1.4709\n",
      "Training loss at step 275: 1.4638\n",
      "Training loss at step 300: 1.6519\n",
      "Training loss at step 325: 1.6029\n",
      "Training loss at step 350: 1.6061\n",
      "Training accuracy: 0.4242 Validation accuracy: 0.4228 Time taken: 1335.31s\n",
      "Epoch 5/200\n",
      "Training loss at step 0: 1.4707\n",
      "Training loss at step 25: 1.5620\n",
      "Training loss at step 50: 1.7185\n",
      "Training loss at step 75: 1.6399\n",
      "Training loss at step 100: 1.4172\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1,nesterov=True)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "history = [[],[],[]]\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch %d/%d\" % (epoch+1,epochs))\n",
    "    if (epoch==100) | (epoch==150):\n",
    "        optimizer.learning_rate = optimizer.learning_rate/10\n",
    "    \n",
    "    start_time = time.time()\n",
    "    step = 0\n",
    "    for x_batch_train, y_batch_train in train_data:\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "        if step % 25 == 0:\n",
    "            print(\n",
    "                \"Training loss at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "        step += 1\n",
    "        if step > len(x_train)/batch_size:\n",
    "            break\n",
    "\n",
    "    history[0].append(loss_value)\n",
    "    train_acc = train_acc_metric.result()\n",
    "    train_acc_metric.reset_states()\n",
    "    \n",
    "    step = 0\n",
    "    for x_batch_val, y_batch_val in validation_data:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "        step += 1\n",
    "        if step > len(x_val)/batch_size:\n",
    "            break\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    history[1].append(train_acc)\n",
    "    history[2].append(val_acc)\n",
    "    print(\"Training accuracy: %.4f\" % (float(train_acc),)\n",
    "          ,\"Validation accuracy: %.4f\" % (float(val_acc),),\"Time taken: %.2fs\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-calgary",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
